{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a14009-0976-41cf-8b28-896dad8c142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9d477fb-a335-4c4f-8dda-9a8e6bfd87b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "from hand_landmarks.neural_networks.mlp import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8518cda9-3ca9-44fe-aeb7-5db5d2ccb806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_landmarks_through_frame(landmarks, time_sleep=0.01):\n",
    "    assert landmarks.shape[1:] == (21, 3)\n",
    "    \n",
    "    x = np.array([[500, 0, 0],\n",
    "                  [0, 0, 0]])\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(x)\n",
    "\n",
    "    lines = [[0, 0]]\n",
    "    colors = [[1, 0, 0] for i in range(len(lines))]\n",
    "    line_set = o3d.geometry.LineSet(\n",
    "        points=o3d.utility.Vector3dVector(x),\n",
    "        lines=o3d.utility.Vector2iVector(lines)\n",
    "    )\n",
    "    line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window()\n",
    "    vis.add_geometry(pcd)\n",
    "    vis.add_geometry(line_set)\n",
    "\n",
    "    for i in range(landmarks.shape[0]):\n",
    "        hand_lmks = landmarks[i]\n",
    "        pcd.points = o3d.utility.Vector3dVector(hand_lmks)\n",
    "\n",
    "        lines = [[0,1],[1,2],[2,3],[3,4], \n",
    "                 [0,5],[5,6],[6,7],[7,8],\n",
    "                 [5,9],[9,10],[10,11],[11,12],\n",
    "                 [9,13],[13,14],[14,15],[15,16],\n",
    "                 [13,17],[17,18],[18,19],[19,20],[0,17]]\n",
    "        colors = [[1, 0, 0] for i in range(len(lines))]\n",
    "        line_set.points = o3d.utility.Vector3dVector(hand_lmks)  # Update the points\n",
    "        line_set.lines = o3d.utility.Vector2iVector(lines)  # Update the lines\n",
    "        line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "        vis.update_geometry(pcd)\n",
    "        vis.update_geometry(line_set)\n",
    "        vis.poll_events()\n",
    "        vis.update_renderer()\n",
    "        \n",
    "        time.sleep(time_sleep)\n",
    "\n",
    "    vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48d24564-93ff-4736-befa-ed69d3be4f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_to_csv(file_name, data, num_cam=2):\n",
    "    num_points_each_joint = 3\n",
    "    num_joints_each_hand = 21\n",
    "    num_input_cols = num_cam * num_points_each_joint * num_joints_each_hand\n",
    "\n",
    "    input_header = input_cam1_header\n",
    "    for i in range(2, num_cam+1):\n",
    "        input_cam_i_header = input_cam1_header.replace(\"cam1\", \"cam{}\".format(i))\n",
    "        input_header += ',' + input_cam_i_header\n",
    "\n",
    "    output_header = input_cam1_header.replace(\"cam1_\", \"\").replace(\"in\", \"out\")\n",
    "    csv_header = input_header + ',' + output_header\n",
    "\n",
    "    assert len(csv_header.split(\",\")) == data.shape[1]\n",
    "\n",
    "    np.savetxt(file_name, data, delimiter=',', fmt='%f', header=csv_header, comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730ed955-9430-4acd-8d71-b80135fc1dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828eb86-e456-41f1-b017-7a1e362793ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe9b2be-6f65-4aaf-be82-d6a78a2359e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d89f3b67-4f80-4c3e-9511-fb851642c913",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Visualize GTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1099b869-630b-46f7-9c8c-b2e881518d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_lmks_file = np.load('/home/giakhang/dev/Hand_pose_estimation_3D/hand_landmarks/data/hand_landmarks_2024_6_14_16_12.npz')\n",
    "hand_lmks_gt = hand_lmks_file[\"landmarks_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61b70cc6-fb51-476c-80db-8e81998dca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_landmarks_through_frame(hand_lmks_gt, time_sleep=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a45058c-9636-4ee5-9490-8987c58203e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9327ef33-afd2-4978-a3d5-bc14cafba95d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Visualize raw landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31b3c3f9-d337-4fd0-baac-96817c8c959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_lmks_file = np.load('/home/giakhang/dev/Hand_pose_estimation_3D/hand_landmarks/data/hand_landmarks_2024_6_14_16_12.npz')\n",
    "hand_lmks_input = hand_lmks_file[\"raw_xyZ_of_opposite_cam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4555a6e7-0727-40c7-94e2-852bb302a25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_landmarks_through_frame(hand_lmks_input, time_sleep=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b8b95bf-449d-418a-a5e0-6659215fc0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_lmks_input = hand_lmks_file[\"raw_xyZ_of_rightside_cam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26e3966f-222c-4089-b983-fe1b98b1f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_landmarks_through_frame(hand_lmks_input, time_sleep=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a05f7e-d2f4-49a9-a684-db618a2ff2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d761dd-2df7-401f-8e20-83e0dd6f86ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a9ec944-8ea0-47fb-944d-079754f43905",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb266333-aa98-40b7-869f-0cc35a5d4397",
   "metadata": {},
   "source": [
    "## Visualize for verifing that we save the correct landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d80ec62a-629d-43a7-bc48-7d4b10f07404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from a CSV file\n",
    "train_data = pd.read_csv('/home/giakhang/dev/Hand_pose_estimation_3D/hand_landmarks/data/train_hand_landmarks_2024_6_14_16_12.csv')\n",
    "\n",
    "num_output_nodes = 21 * 3\n",
    "X_train = train_data.iloc[:, :-(num_output_nodes)]\n",
    "Y_train = train_data.iloc[:, -(num_output_nodes):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bb38a17-a3f6-4394-92db-a0ecda8bebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_landmarks_each_cam = 21 * 3\n",
    "X_train_cam_1 = X_train.values[:, :num_landmarks_each_cam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f5c6a9c-85eb-40c6-8ba9-eea8d957f84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_landmarks_through_frame(X_train_cam_1.reshape(-1, 21, 3), time_sleep=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74c3ac83-5711-49d7-802b-f594bfc3544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_landmarks_through_frame(Y_train.values.reshape(-1, 21, 3), time_sleep=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "426823a6-d610-4509-b354-af876167a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cam_2 = X_train.values[:, num_landmarks_each_cam:]\n",
    "visualize_landmarks_through_frame(X_train_cam_2.reshape(-1, 21, 3), time_sleep=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b7782-a716-444a-b48e-2035047352bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62dad440-9909-4f8d-99a0-9f56c4d8a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from a CSV file\n",
    "test_data = pd.read_csv('/home/giakhang/dev/Hand_pose_estimation_3D/hand_landmarks/data/test_hand_landmarks_2024_6_14_16_12.csv')\n",
    "\n",
    "num_output_nodes = 21 * 3\n",
    "X_test = test_data.iloc[:, :-(num_output_nodes)]\n",
    "Y_test = test_data.iloc[:, -(num_output_nodes):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "987e9b76-d813-4277-a67d-4dcd7ea0eb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_landmarks_each_cam = 21 * 3\n",
    "X_test_cam_1 = X_test.values[:, :num_landmarks_each_cam]\n",
    "visualize_landmarks_through_frame(X_test_cam_1.reshape(-1, 21, 3), time_sleep=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3e2d352-cd48-422c-b3fe-96a156499c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cam_2 = X_test.values[:, num_landmarks_each_cam:]\n",
    "visualize_landmarks_through_frame(X_test_cam_2.reshape(-1, 21, 3), time_sleep=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f917bc59-542b-4d85-8e21-b64d1aa9735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_landmarks_through_frame(Y_test.values.reshape(-1, 21, 3), time_sleep=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba830f7-832c-4e0c-b837-fca57760c64a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc478798-bc6f-441f-a15f-36cbea8524f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec01fcde-cea7-4538-b538-8bca8ea52938",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_landmarks = 21 * 3\n",
    "landmarks_opposite_cam = X.values[:, :num_landmarks]\n",
    "landmarks_rightside_cam = X.values[:, num_landmarks:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "303eaeaf-8a4b-404d-8080-64b6d3f05d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_opposite_cam = landmarks_opposite_cam.reshape(landmarks_opposite_cam.shape[0], 21, -1)\n",
    "landmarks_rightside_cam = landmarks_rightside_cam.reshape(landmarks_rightside_cam.shape[0], 21, -1)\n",
    "landmarks_gt = Y.values\n",
    "landmarks_gt = landmarks_gt.reshape(landmarks_gt.shape[0], 21, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f73cf65-6df8-485c-8e26-4e1b8f62a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_landmarks_through_frame(landmarks_rightside_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d227d-a274-494e-b0cb-5e22cbe47704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc55ef-d38e-469c-aefd-502a0f90bd5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caccf9cb-952a-460c-963e-3dc43ea440a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from a CSV file\n",
    "train_data = pd.read_csv('/home/giakhang/dev/Hand_pose_estimation_3D/hand_landmarks/data/train_hand_landmarks_2024_6_14_16_12.csv')\n",
    "test_data = pd.read_csv('/home/giakhang/dev/Hand_pose_estimation_3D/hand_landmarks/data/test_hand_landmarks_2024_6_14_16_12.csv')\n",
    "\n",
    "num_output_nodes = 21 * 3\n",
    "X_train = train_data.iloc[:, :-(num_output_nodes)]\n",
    "Y_train = train_data.iloc[:, -(num_output_nodes):]\n",
    "X_test = test_data.iloc[:, :-(num_output_nodes)]\n",
    "Y_test = test_data.iloc[:, -(num_output_nodes):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7aee469-9e57-4889-bd07-5e83ef7ee7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (400, 126)\n",
      "Y_train shape:  (400, 63)\n",
      "X_test shape:  (100, 126)\n",
      "Y_test shape:  (100, 63)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"Y_train shape: \", Y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"Y_test shape: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9427f15a-6cdd-49be-a8c2-77a4373da606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MinMaxScaler for scaling between 0 and 1\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_Y = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c985f11c-1890-4d6f-8af5-4390c590077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the scaler on the data and transform\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "Y_train_scaled = scaler_Y.fit_transform(Y_train)\n",
    "Y_test_scaled = scaler_Y.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d074bc6a-c831-44b6-9d13-3a2c8cf84834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train_scaled, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test_scaled, dtype=torch.float32)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32  # Adjust according to your needs\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ebbeabc-3c0e-4cf4-b8a4-6e56f7f8c3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/giakhang/dev/Hand_pose_estimation_3D/hand_landmarks/neural_networks/scaler/scaler_X.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler_X, \"/home/giakhang/dev/Hand_pose_estimation_3D/hand_landmarks/neural_networks/scaler/scaler_X.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26246d9b-50bf-4820-95ad-1b9b74701f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/giakhang/dev/Hand_pose_estimation_3D/hand_landmarks/neural_networks/scaler/scaler_Y.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler_Y, \"/home/giakhang/dev/Hand_pose_estimation_3D/hand_landmarks/neural_networks/scaler/scaler_Y.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8334987b-31e8-4a28-aad8-e76cd3b6da18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e038ee58-cb01-43e8-9e52-8b8f47ae65ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  torch.Size([400, 126])\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape: ', X_train_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0b86b4e-3851-4475-93f3-c41506a310c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape:  torch.Size([100, 126])\n"
     ]
    }
   ],
   "source": [
    "print('X_test shape: ', X_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b845c10a-1c8f-4f2c-abde-76e7aff49569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7569a7eb-8947-4bba-bc31-aecd260d397e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a547309a-6500-4010-a516-dea228cc3a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59ee2f92-9018-419e-b2b3-7ca8638ac342",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22be5adc-1633-43f4-93b6-135a6467f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "\n",
    "# Define your criterion and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57872b65-fa8b-4a12-9a11-f3acc0c76967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Train Loss: 0.0853, Val Loss: 0.0306\n",
      "Epoch [501/10000], Train Loss: 0.0018, Val Loss: 0.0204\n",
      "Epoch [1001/10000], Train Loss: 0.0022, Val Loss: 0.0094\n",
      "Epoch [1501/10000], Train Loss: 0.0019, Val Loss: 0.0423\n",
      "Epoch [2001/10000], Train Loss: 0.0022, Val Loss: 0.0283\n",
      "Epoch [2501/10000], Train Loss: 0.0026, Val Loss: 0.0294\n",
      "Epoch [3001/10000], Train Loss: 0.0032, Val Loss: 0.0412\n",
      "Epoch [3501/10000], Train Loss: 0.0040, Val Loss: 0.0198\n",
      "Epoch [4001/10000], Train Loss: 0.0053, Val Loss: 0.0082\n",
      "Epoch [4501/10000], Train Loss: 0.0031, Val Loss: 0.0081\n",
      "Epoch [5001/10000], Train Loss: 0.0068, Val Loss: 0.0115\n",
      "Epoch [5501/10000], Train Loss: 0.0076, Val Loss: 0.0114\n",
      "Epoch [6001/10000], Train Loss: 0.0065, Val Loss: 0.0223\n",
      "Epoch [6501/10000], Train Loss: 0.0055, Val Loss: 0.0123\n",
      "Epoch [7001/10000], Train Loss: 0.0055, Val Loss: 0.0118\n",
      "Epoch [7501/10000], Train Loss: 0.0059, Val Loss: 0.0119\n",
      "Epoch [8001/10000], Train Loss: 0.0058, Val Loss: 0.0115\n",
      "Epoch [8501/10000], Train Loss: 0.0100, Val Loss: 0.0170\n",
      "Epoch [9001/10000], Train Loss: 0.0092, Val Loss: 0.0149\n",
      "Epoch [9501/10000], Train Loss: 0.0110, Val Loss: 0.0161\n",
      "Epoch [10000/10000], Train Loss: 0.0106, Val Loss: 0.0158\n",
      "Best model found at epoch 4501, with validation loss: 0.0081\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to track best model\n",
    "best_loss = float('inf')\n",
    "best_epoch = 0\n",
    "num_epochs = 10000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate average training loss\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Print training and validation loss\n",
    "    if epoch % 500 == 0 or epoch == num_epochs - 1:\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs_val, labels_val in test_loader:\n",
    "                inputs_val, labels_val = inputs_val.to(device), labels_val.to(device)\n",
    "\n",
    "                outputs_val = model(inputs_val)\n",
    "                loss_val = criterion(outputs_val, labels_val)\n",
    "                val_loss += loss_val.item()\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save best model based on validation loss\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), '/home/giakhang/dev/Hand_pose_estimation_3D/hand_landmarks/neural_networks/weights/best_model.pth')\n",
    "\n",
    "print(f'Best model found at epoch {best_epoch+1}, with validation loss: {best_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2949b997-dd46-4cd3-a8b3-1fccffdad967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a371a420-58c7-4fba-9bba-8ab9c4c66e8d",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0537d-f695-430e-8b9b-2ad9bf558e39",
   "metadata": {},
   "source": [
    "Evaluate with raw predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b7b94ab-fca8-4969-8e08-9798f707c823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from mlp_model.pth\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model state dictionary\n",
    "model = MLP()\n",
    "model.load_state_dict(torch.load('/home/giakhang/dev/Hand_pose_estimation_3D/hand_landmarks/neural_networks/weights/best_model.pth'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "model.to(device)\n",
    "print('Model loaded from mlp_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44df06d5-6857-4192-8f42-784f699e9181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss of the model on the test set: 0.0081\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    print(f'Average Loss of the model on the test set: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0add39-2edf-4f75-b7d8-fc7e4276c036",
   "metadata": {},
   "source": [
    "Evaluate with scaled predictions (error unit: mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09ed8ee9-6d8b-4952-9d13-f9e1348694dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = X_test_tensor.to(device)\n",
    "predictions = model(X_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2701053b-19d3-4bd6-8cdf-8ef5901f3d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.detach().to(\"cpu\").numpy()\n",
    "predictions_in_mm = scaler_Y.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13262655-cd58-4f26-9c41-24b1abfb8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_tensor = scaler_Y.inverse_transform(Y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5c63c78-4c9b-4a8c-a9bd-8583419090e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106.98236877170699"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(Y_test_tensor, predictions_in_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a86ccff-adba-41df-a075-983a576866f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "485a32eb-4bcc-4182-851f-44ddcc1118dd",
   "metadata": {},
   "source": [
    "# Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8db5eb2-dca9-443f-9802-bfafc03d6fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_in_mm = predictions_in_mm.reshape(predictions_in_mm.shape[0], 21, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "757bc96b-a55a-4f51-a564-ffa9c11d6c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_landmarks_through_frame(predictions_in_mm, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a6f3b-70d4-443d-81b4-d0ff21c312af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e5aad8-9cf4-47d0-998e-606f15beb65a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4237da95-4baf-4b08-b280-536688086f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d60113d-fad5-4b3e-a536-7574f1c3d72e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
