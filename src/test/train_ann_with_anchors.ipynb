{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/giakhang/dev/pose_sandbox\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.abspath(os.curdir),\n",
    "                \"Hand_pose_estimation_3D/arm_and_hand\"))\n",
    "sys.path.append(os.path.join(os.path.abspath(os.curdir),\n",
    "                \"Hand_pose_estimation_3D\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from ann import ANN\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from csv_writer import columns_to_normalize, fusion_csv_columns_name\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "from landmarks_scaler import LandmarksScaler\n",
    "from train_ann_with_anchors import train_model\n",
    "from dataloader_hand_only_ann_with_anchors import HandLandmarksDataset_ANN_With_Anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hand_lmks = 21\n",
    "num_hand_anchors = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT_DIM = (num_hand_lmks * 3 * 2) + (5 * 3)  + (num_hand_anchors * 3)\n",
    "INPUT_DIM = (num_hand_lmks * 3 * 2) \n",
    "OUTPUT_DIM = (num_hand_lmks - num_hand_anchors) * 3\n",
    "HIDDEN_DIM_CONTAINER = [num_hand_lmks * 3 * 2, 100, 100, 100, 70, 70]\n",
    "NUM_HIDDEN_LAYERS = 5\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "model = ANN(input_dim=INPUT_DIM,\n",
    "            output_dim=OUTPUT_DIM,\n",
    "            hidden_dim_container=HIDDEN_DIM_CONTAINER,\n",
    "            num_hidden_layers=NUM_HIDDEN_LAYERS,\n",
    "            dropout_rate=DROPOUT_RATE)\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"ann_left_hand\"\n",
    "DATETIME = \"{}\".format(datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "DATE = \"{}\".format(datetime.now().strftime(\"%Y%m%d\"))\n",
    "BASE_DIR = \"Hand_pose_estimation_3D/arm_and_hand/runs/{}\".format(MODEL_NAME)\n",
    "SAVE_DIR = os.path.join(BASE_DIR, DATE, DATETIME)\n",
    "DATA_DIR = \"data\"  \n",
    "writer = SummaryWriter(log_dir=SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_hand_fused_names = [\"left shoulder\", \"left elbow\", \"left hip\", \"right shoulder\",\n",
    "    \"right hip\", \"WRIST\", \"THUMB_CMC\", \"THUMB_MCP\", \"THUMB_IP\", \n",
    "    \"THUMB_TIP\", \"INDEX_FINGER_MCP\", \"INDEX_FINGER_PIP\", \"INDEX_FINGER_DIP\",\n",
    "    \"INDEX_FINGER_TIP\", \"MIDDLE_FINGER_MCP\", \"MIDDLE_FINGER_PIP\", \"MIDDLE_FINGER_DIP\",\n",
    "    \"MIDDLE_FINGER_TIP\", \"RING_FINGER_MCP\", \"RING_FINGER_PIP\", \"RING_FINGER_DIP\",\n",
    "    \"RING_FINGER_TIP\", \"PINKY_MCP\", \"PINKY_PIP\", \"PINKY_DIP\", \"PINKY_TIP\", \"right elbow\",\n",
    "    \"RIGHT_WRIST\", \"RIGHT_THUMB_CMC\", \"RIGHT_THUMB_MCP\", \"RIGHT_THUMB_IP\", \"RIGHT_THUMB_TIP\",\n",
    "    \"RIGHT_INDEX_FINGER_MCP\", \"RIGHT_INDEX_FINGER_PIP\", \"RIGHT_INDEX_FINGER_DIP\",\n",
    "    \"RIGHT_INDEX_FINGER_TIP\", \"RIGHT_MIDDLE_FINGER_MCP\", \"RIGHT_MIDDLE_FINGER_PIP\",\n",
    "    \"RIGHT_MIDDLE_FINGER_DIP\", \"RIGHT_MIDDLE_FINGER_TIP\", \"RIGHT_RING_FINGER_MCP\",\n",
    "    \"RIGHT_RING_FINGER_PIP\", \"RIGHT_RING_FINGER_DIP\", \"RIGHT_RING_FINGER_TIP\",\n",
    "    \"RIGHT_PINKY_MCP\", \"RIGHT_PINKY_PIP\", \"RIGHT_PINKY_DIP\", \"RIGHT_PINKY_TIP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"  \n",
    "SELECTED_DATE = \"2024-*\"  # Keep '*' when using glob.glob\n",
    "\n",
    "train_paths = glob.glob(os.path.join(DATA_DIR, \"{}/{}/fine_landmarks_{}_*.csv\".format(SELECTED_DATE, SELECTED_DATE, \"train\")))\n",
    "val_paths = glob.glob(os.path.join(DATA_DIR, \"{}/{}/fine_landmarks_{}_*.csv\".format(SELECTED_DATE, SELECTED_DATE, \"val\")))\n",
    "\n",
    "body_lines = [[0,2], [0, 3], [2, 4], [3, 4]]\n",
    "lefthand_lines = [[0, 1], [1, 5], [5, 6], [5, 10], [5, 22], [10, 14], [14, 18], [18, 22], \n",
    "    [6, 7], [7, 8], [8, 9], \n",
    "    [10, 11], [11, 12], [12, 13], \n",
    "    [14, 15], [15, 16], [16, 17], \n",
    "    [18, 19], [19, 20], [20, 21], \n",
    "    [22, 23], [23, 24], [24, 25]]\n",
    "train_body_distance_thres = 550\n",
    "train_leftarm_distance_thres = 550\n",
    "train_lefthand_distance_thres = 200\n",
    "val_body_distance_thres = 450\n",
    "val_leftarm_distance_thres = 450\n",
    "val_lefthand_distance_thres = 150\n",
    "\n",
    "input_scaler = MinMaxScaler()\n",
    "output_scaler = MinMaxScaler()\n",
    "\n",
    "train_dataset = HandLandmarksDataset_ANN_With_Anchors(train_paths, \n",
    "    arm_hand_fused_names,\n",
    "    body_lines, \n",
    "    lefthand_lines, \n",
    "    train_body_distance_thres, \n",
    "    train_leftarm_distance_thres, \n",
    "    train_lefthand_distance_thres,\n",
    "    filter_outlier=True,\n",
    "    only_keep_frames_contain_lefthand=True,\n",
    "    cvt_normalized_xy_to_XY=True,\n",
    "    use_thumb_as_anchor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hand_pose_estimation_3D/arm_and_hand/runs/ann_left_hand/20241108/20241108-1450/output_scaler.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_scaler.fit_transform(train_dataset._inputs)\n",
    "output_scaler.fit_transform(train_dataset._outputs)\n",
    "\n",
    "input_scaler_save_path = os.path.join(SAVE_DIR, \"input_scaler.pkl\")\n",
    "output_scaler_save_path = os.path.join(SAVE_DIR, \"output_scaler.pkl\")\n",
    "\n",
    "joblib.dump(input_scaler, input_scaler_save_path)\n",
    "joblib.dump(output_scaler, output_scaler_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_scaler = LandmarksScaler(scaler_path=input_scaler_save_path)\n",
    "output_scaler = LandmarksScaler(scaler_path=output_scaler_save_path)\n",
    "\n",
    "train_dataset = HandLandmarksDataset_ANN_With_Anchors(train_paths, \n",
    "    arm_hand_fused_names,\n",
    "    body_lines, \n",
    "    lefthand_lines, \n",
    "    train_body_distance_thres, \n",
    "    train_leftarm_distance_thres, \n",
    "    train_lefthand_distance_thres,\n",
    "    filter_outlier=True,\n",
    "    only_keep_frames_contain_lefthand=True,\n",
    "    cvt_normalized_xy_to_XY=True,\n",
    "    use_thumb_as_anchor=False,\n",
    "    input_scaler=input_scaler,\n",
    "    output_scaler=output_scaler)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=20000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = HandLandmarksDataset_ANN_With_Anchors(val_paths,\n",
    "    arm_hand_fused_names,\n",
    "    body_lines,\n",
    "    lefthand_lines,\n",
    "    val_body_distance_thres,\n",
    "    val_leftarm_distance_thres,\n",
    "    val_lefthand_distance_thres,\n",
    "    filter_outlier=True,\n",
    "    only_keep_frames_contain_lefthand=True,\n",
    "    cvt_normalized_xy_to_XY=True,\n",
    "    use_thumb_as_anchor=False,\n",
    "    input_scaler=input_scaler,\n",
    "    output_scaler=output_scaler)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1000, shuffle=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 10000\n",
    "current_time = datetime.now().strftime('%Y%m%d-%H%M')\n",
    "save_path = os.path.join(SAVE_DIR, \"{}_{}_layers_best.pth\".format(MODEL_NAME, NUM_HIDDEN_LAYERS))\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', \n",
    "    factor=math.sqrt(0.1), patience=500, verbose=True, min_lr=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved with Validation Loss: 0.9352\n",
      "Model saved with Validation Loss: 0.7429\n",
      "Model saved with Validation Loss: 0.6085\n",
      "Model saved with Validation Loss: 0.5157\n",
      "Model saved with Validation Loss: 0.4573\n",
      "Model saved with Validation Loss: 0.4216\n",
      "Model saved with Validation Loss: 0.3973\n",
      "Model saved with Validation Loss: 0.3815\n",
      "Model saved with Validation Loss: 0.3704\n",
      "Model saved with Validation Loss: 0.3621\n",
      "Model saved with Validation Loss: 0.3538\n",
      "Model saved with Validation Loss: 0.3479\n",
      "Model saved with Validation Loss: 0.3429\n",
      "Model saved with Validation Loss: 0.3395\n",
      "Model saved with Validation Loss: 0.3349\n",
      "Model saved with Validation Loss: 0.3312\n",
      "Model saved with Validation Loss: 0.3306\n",
      "Model saved with Validation Loss: 0.3253\n",
      "Model saved with Validation Loss: 0.3242\n",
      "Model saved with Validation Loss: 0.3223\n",
      "Model saved with Validation Loss: 0.3171\n",
      "Model saved with Validation Loss: 0.3096\n",
      "Model saved with Validation Loss: 0.3056\n",
      "Model saved with Validation Loss: 0.2999\n",
      "Model saved with Validation Loss: 0.2964\n",
      "Model saved with Validation Loss: 0.2942\n",
      "Model saved with Validation Loss: 0.2804\n",
      "Model saved with Validation Loss: 0.2733\n",
      "Model saved with Validation Loss: 0.2655\n",
      "Model saved with Validation Loss: 0.2564\n",
      "Epoch 50/10000, Training Loss: 0.2384\n",
      "Epoch 50/10000, Validation Loss: 0.2492\n",
      "Model saved with Validation Loss: 0.2492\n",
      "Model saved with Validation Loss: 0.2467\n",
      "Model saved with Validation Loss: 0.2462\n",
      "Model saved with Validation Loss: 0.2441\n",
      "Model saved with Validation Loss: 0.2356\n",
      "Model saved with Validation Loss: 0.2190\n",
      "Model saved with Validation Loss: 0.2118\n",
      "Model saved with Validation Loss: 0.1752\n",
      "Model saved with Validation Loss: 0.1669\n",
      "Model saved with Validation Loss: 0.1614\n",
      "Epoch 100/10000, Training Loss: 0.1930\n",
      "Epoch 100/10000, Validation Loss: 0.2667\n",
      "Model saved with Validation Loss: 0.1592\n",
      "Model saved with Validation Loss: 0.1558\n",
      "Model saved with Validation Loss: 0.1536\n",
      "Model saved with Validation Loss: 0.1512\n",
      "Model saved with Validation Loss: 0.1426\n",
      "Model saved with Validation Loss: 0.1397\n",
      "Epoch 150/10000, Training Loss: 0.1584\n",
      "Epoch 150/10000, Validation Loss: 0.2324\n",
      "Model saved with Validation Loss: 0.1355\n",
      "Model saved with Validation Loss: 0.1232\n",
      "Model saved with Validation Loss: 0.1194\n",
      "Epoch 200/10000, Training Loss: 0.1422\n",
      "Epoch 200/10000, Validation Loss: 0.1310\n",
      "Model saved with Validation Loss: 0.1131\n",
      "Model saved with Validation Loss: 0.1117\n",
      "Model saved with Validation Loss: 0.1095\n",
      "Epoch 250/10000, Training Loss: 0.1251\n",
      "Epoch 250/10000, Validation Loss: 0.1143\n",
      "Model saved with Validation Loss: 0.1050\n",
      "Epoch 300/10000, Training Loss: 0.1198\n",
      "Epoch 300/10000, Validation Loss: 0.1237\n",
      "Model saved with Validation Loss: 0.0986\n",
      "Epoch 350/10000, Training Loss: 0.1177\n",
      "Epoch 350/10000, Validation Loss: 0.1090\n",
      "Model saved with Validation Loss: 0.0946\n",
      "Epoch 400/10000, Training Loss: 0.1155\n",
      "Epoch 400/10000, Validation Loss: 0.1428\n",
      "Model saved with Validation Loss: 0.0940\n",
      "Model saved with Validation Loss: 0.0932\n",
      "Model saved with Validation Loss: 0.0923\n",
      "Epoch 450/10000, Training Loss: 0.1101\n",
      "Epoch 450/10000, Validation Loss: 0.1171\n",
      "Epoch 500/10000, Training Loss: 0.1114\n",
      "Epoch 500/10000, Validation Loss: 0.1820\n",
      "Epoch 550/10000, Training Loss: 0.1063\n",
      "Epoch 550/10000, Validation Loss: 0.1060\n",
      "Model saved with Validation Loss: 0.0867\n",
      "Epoch 600/10000, Training Loss: 0.1042\n",
      "Epoch 600/10000, Validation Loss: 0.1066\n",
      "Model saved with Validation Loss: 0.0866\n",
      "Model saved with Validation Loss: 0.0845\n",
      "Epoch 650/10000, Training Loss: 0.1044\n",
      "Epoch 650/10000, Validation Loss: 0.0954\n",
      "Model saved with Validation Loss: 0.0833\n",
      "Epoch 700/10000, Training Loss: 0.1017\n",
      "Epoch 700/10000, Validation Loss: 0.1364\n",
      "Epoch 750/10000, Training Loss: 0.0996\n",
      "Epoch 750/10000, Validation Loss: 0.1195\n",
      "Epoch 800/10000, Training Loss: 0.0998\n",
      "Epoch 800/10000, Validation Loss: 0.1052\n",
      "Model saved with Validation Loss: 0.0808\n",
      "Epoch 850/10000, Training Loss: 0.0980\n",
      "Epoch 850/10000, Validation Loss: 0.0839\n",
      "Model saved with Validation Loss: 0.0769\n",
      "Epoch 900/10000, Training Loss: 0.0990\n",
      "Epoch 900/10000, Validation Loss: 0.1077\n",
      "Epoch 950/10000, Training Loss: 0.0988\n",
      "Epoch 950/10000, Validation Loss: 0.1107\n",
      "Model saved with Validation Loss: 0.0748\n",
      "Epoch 1000/10000, Training Loss: 0.0996\n",
      "Epoch 1000/10000, Validation Loss: 0.1242\n",
      "Epoch 1050/10000, Training Loss: 0.0973\n",
      "Epoch 1050/10000, Validation Loss: 0.0819\n",
      "Epoch 1100/10000, Training Loss: 0.0954\n",
      "Epoch 1100/10000, Validation Loss: 0.1188\n",
      "Model saved with Validation Loss: 0.0745\n",
      "Model saved with Validation Loss: 0.0730\n",
      "Epoch 1150/10000, Training Loss: 0.0959\n",
      "Epoch 1150/10000, Validation Loss: 0.0706\n",
      "Model saved with Validation Loss: 0.0706\n",
      "Epoch 1200/10000, Training Loss: 0.0945\n",
      "Epoch 1200/10000, Validation Loss: 0.0829\n",
      "Epoch 1250/10000, Training Loss: 0.0960\n",
      "Epoch 1250/10000, Validation Loss: 0.1028\n",
      "Epoch 1300/10000, Training Loss: 0.0933\n",
      "Epoch 1300/10000, Validation Loss: 0.0846\n",
      "Epoch 1350/10000, Training Loss: 0.0965\n",
      "Epoch 1350/10000, Validation Loss: 0.0784\n",
      "Epoch 1400/10000, Training Loss: 0.0964\n",
      "Epoch 1400/10000, Validation Loss: 0.0957\n",
      "Epoch 1450/10000, Training Loss: 0.0947\n",
      "Epoch 1450/10000, Validation Loss: 0.1080\n",
      "Epoch 1500/10000, Training Loss: 0.1047\n",
      "Epoch 1500/10000, Validation Loss: 0.1291\n",
      "Epoch 1550/10000, Training Loss: 0.0957\n",
      "Epoch 1550/10000, Validation Loss: 0.1387\n",
      "Epoch 1600/10000, Training Loss: 0.0957\n",
      "Epoch 1600/10000, Validation Loss: 0.1257\n",
      "Epoch 1650/10000, Training Loss: 0.0947\n",
      "Epoch 1650/10000, Validation Loss: 0.0869\n",
      "Epoch 01651: reducing learning rate of group 0 to 3.1623e-04.\n",
      "Model saved with Validation Loss: 0.0688\n",
      "Model saved with Validation Loss: 0.0685\n",
      "Model saved with Validation Loss: 0.0681\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_seq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/dev/pose_sandbox/Hand_pose_estimation_3D/arm_and_hand/train_ann_with_anchors.py:248\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, num_epochs, save_path, scheduler, writer, log_seq)\u001b[0m\n\u001b[1;32m    246\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    247\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m    249\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mreshape(inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    250\u001b[0m     hand_anchors \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:]  \u001b[38;5;66;03m# (B, 3, 3)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pose_sandbox/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/pose_sandbox/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/pose_sandbox/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/pose_sandbox/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/dev/pose_sandbox/Hand_pose_estimation_3D/arm_and_hand/dataloader_ann.py:254\u001b[0m, in \u001b[0;36mHandArmLandmarksDataset_ANN.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    251\u001b[0m input_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inputs[idx]\n\u001b[1;32m    252\u001b[0m output_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs[idx]\n\u001b[0;32m--> 254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mtensor(output_row, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train_model(model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    optimizer, \n",
    "    num_epochs=num_epochs, \n",
    "    save_path=save_path,\n",
    "    scheduler=scheduler,\n",
    "    writer=writer,\n",
    "    log_seq=50)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose_sandbox",
   "language": "python",
   "name": "pose_sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
