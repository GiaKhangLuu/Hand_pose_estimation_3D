{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/giakhang/dev/pose_sandbox\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.abspath(os.curdir),\n",
    "                \"Hand_pose_estimation_3D/arm_and_hand\"))\n",
    "sys.path.append(os.path.join(os.path.abspath(os.curdir),\n",
    "                \"Hand_pose_estimation_3D\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from transformer_encoder import TransformerEncoder\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from ann import ANN\n",
    "from dataloader_ann import HandArmLandmarksDataset_ANN\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from csv_writer import columns_to_normalize, fusion_csv_columns_name\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "from landmarks_scaler import LandmarksScaler\n",
    "from train_ann_no_intrinsics import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "thumb_landmarks = [\"left shoulder\", \"left hip\", \"right shoulder\", \"right hip\", \n",
    "    \"left elbow\", \"WRIST\", \n",
    "    \"THUMB_CMC\", \"INDEX_FINGER_MCP\", \"MIDDLE_FINGER_MCP\", \"RING_FINGER_MCP\", \"PINKY_MCP\",\n",
    "    \"THUMB_TIP\", \"INDEX_FINGER_TIP\", \"MIDDLE_FINGER_TIP\", \"RING_FINGER_TIP\", \"PINKY_TIP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 144 + 144 + (len(thumb_landmarks) * 3)\n",
    "OUTPUT_DIM = 144 \n",
    "HIDDEN_DIM = int(144 + (144 / 2))\n",
    "NUM_HIDDEN_LAYERS = 4\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "model = ANN(input_dim=INPUT_DIM,\n",
    "            output_dim=OUTPUT_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            num_hidden_layers=NUM_HIDDEN_LAYERS,\n",
    "            dropout_rate=DROPOUT_RATE)\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"ann\"\n",
    "DATETIME = \"{}\".format(datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "DATE = \"{}\".format(datetime.now().strftime(\"%Y%m%d\"))\n",
    "BASE_DIR = \"/home/giakhang/dev/pose_sandbox/Hand_pose_estimation_3D/arm_and_hand/runs/{}\".format(MODEL_NAME)\n",
    "SAVE_DIR = os.path.join(BASE_DIR, DATE, DATETIME)\n",
    "DATA_DIR = \"/home/giakhang/dev/pose_sandbox/data\"  \n",
    "writer = SummaryWriter(log_dir=SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_DATE = \"2024-*\"\n",
    "train_paths = glob.glob(os.path.join(DATA_DIR, \"{}/{}/fine_landmarks_{}_*.csv\".format(SELECTED_DATE, SELECTED_DATE, \"train\")))\n",
    "val_paths = glob.glob(os.path.join(DATA_DIR, \"{}/{}/fine_landmarks_{}_*.csv\".format(SELECTED_DATE, SELECTED_DATE, \"val\")))\n",
    "body_lines = [[0,2], [0, 3], [2, 4], [3, 4]]\n",
    "lefthand_lines = [[0, 1], [1, 5], [5, 6], \n",
    "                  [5, 10], [5, 22], [10, 14], \n",
    "                  [14, 18], [18, 22], [6, 7], \n",
    "                  [7, 8], [8, 9], [10, 11], \n",
    "                  [11, 12], [12, 13], [14, 15], \n",
    "                  [15, 16], [16, 17], [18, 19], \n",
    "                  [19, 20], [20, 21], [22, 23], \n",
    "                  [23, 24], [24, 25]]\n",
    "train_body_distance_thres = 550\n",
    "train_leftarm_distance_thres = 550\n",
    "train_lefthand_distance_thres = 200\n",
    "val_body_distance_thres=450\n",
    "val_leftarm_distance_thres=450\n",
    "val_lefthand_distance_thres=150\n",
    "\n",
    "arm_hand_fused_names = [\"left shoulder\", \"left elbow\", \"left hip\", \"right shoulder\",\n",
    "    \"right hip\", \"WRIST\", \"THUMB_CMC\", \"THUMB_MCP\", \"THUMB_IP\", \n",
    "    \"THUMB_TIP\", \"INDEX_FINGER_MCP\", \"INDEX_FINGER_PIP\", \"INDEX_FINGER_DIP\",\n",
    "    \"INDEX_FINGER_TIP\", \"MIDDLE_FINGER_MCP\", \"MIDDLE_FINGER_PIP\", \"MIDDLE_FINGER_DIP\",\n",
    "    \"MIDDLE_FINGER_TIP\", \"RING_FINGER_MCP\", \"RING_FINGER_PIP\", \"RING_FINGER_DIP\",\n",
    "    \"RING_FINGER_TIP\", \"PINKY_MCP\", \"PINKY_PIP\", \"PINKY_DIP\", \"PINKY_TIP\", \"right elbow\",\n",
    "    \"RIGHT_WRIST\", \"RIGHT_THUMB_CMC\", \"RIGHT_THUMB_MCP\", \"RIGHT_THUMB_IP\", \"RIGHT_THUMB_TIP\",\n",
    "    \"RIGHT_INDEX_FINGER_MCP\", \"RIGHT_INDEX_FINGER_PIP\", \"RIGHT_INDEX_FINGER_DIP\",\n",
    "    \"RIGHT_INDEX_FINGER_TIP\", \"RIGHT_MIDDLE_FINGER_MCP\", \"RIGHT_MIDDLE_FINGER_PIP\",\n",
    "    \"RIGHT_MIDDLE_FINGER_DIP\", \"RIGHT_MIDDLE_FINGER_TIP\", \"RIGHT_RING_FINGER_MCP\",\n",
    "    \"RIGHT_RING_FINGER_PIP\", \"RIGHT_RING_FINGER_DIP\", \"RIGHT_RING_FINGER_TIP\",\n",
    "    \"RIGHT_PINKY_MCP\", \"RIGHT_PINKY_PIP\", \"RIGHT_PINKY_DIP\", \"RIGHT_PINKY_TIP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/giakhang/dev/pose_sandbox/Hand_pose_estimation_3D/arm_and_hand/runs/ann/20241021/20241021-1343/input_scaler.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the true dataset to get the scaler then pass the scaler to the true and fake dataset\n",
    "minmax_scaler = MinMaxScaler()\n",
    "train_dataset = HandArmLandmarksDataset_ANN(train_paths, \n",
    "    arm_hand_fused_names,\n",
    "    body_lines, \n",
    "    lefthand_lines, \n",
    "    train_body_distance_thres, \n",
    "    train_leftarm_distance_thres, \n",
    "    train_lefthand_distance_thres,\n",
    "    filter_outlier=True,\n",
    "    only_keep_frames_contain_lefthand=True,\n",
    "    cvt_normalized_xy_to_XY=True,\n",
    "    use_fused_thumb_as_input=True)\n",
    "minmax_scaler.fit_transform(train_dataset._inputs)\n",
    "scaler_save_path = os.path.join(SAVE_DIR, \"input_scaler.pkl\")\n",
    "joblib.dump(minmax_scaler, scaler_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = LandmarksScaler(scaler_path=scaler_save_path)\n",
    "train_dataset = HandArmLandmarksDataset_ANN(train_paths, \n",
    "    arm_hand_fused_names,\n",
    "    body_lines, \n",
    "    lefthand_lines, \n",
    "    train_body_distance_thres, \n",
    "    train_leftarm_distance_thres, \n",
    "    train_lefthand_distance_thres,\n",
    "    filter_outlier=True,\n",
    "    only_keep_frames_contain_lefthand=True,\n",
    "    scaler=scaler,\n",
    "    cvt_normalized_xy_to_XY=True,\n",
    "    use_fused_thumb_as_input=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_dataset = HandArmLandmarksDataset_ANN(val_paths,\n",
    "    arm_hand_fused_names,\n",
    "    body_lines,\n",
    "    lefthand_lines,\n",
    "    val_body_distance_thres,\n",
    "    val_leftarm_distance_thres,\n",
    "    val_lefthand_distance_thres,\n",
    "    filter_outlier=True,\n",
    "    only_keep_frames_contain_lefthand=True,\n",
    "    scaler=scaler,\n",
    "    cvt_normalized_xy_to_XY=True,\n",
    "    use_fused_thumb_as_input=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giakhang/py_venv/mmpose/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 50000\n",
    "current_time = datetime.now().strftime('%Y%m%d-%H%M')\n",
    "save_path = os.path.join(SAVE_DIR, \"{}_{}_layers_best.pth\".format(MODEL_NAME, NUM_HIDDEN_LAYERS))\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', \n",
    "    factor=math.sqrt(0.1), patience=1000, verbose=True, min_lr=1e-8)\n",
    "early_stopping = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved with Validation Loss: 1566345.6500\n",
      "Model saved with Validation Loss: 1555939.0125\n",
      "Model saved with Validation Loss: 1554306.7500\n",
      "Model saved with Validation Loss: 1550695.5375\n",
      "Model saved with Validation Loss: 1546532.6375\n",
      "Model saved with Validation Loss: 1541366.6125\n",
      "Model saved with Validation Loss: 1541038.9875\n",
      "Model saved with Validation Loss: 1535055.6000\n",
      "Model saved with Validation Loss: 1523663.6375\n",
      "Model saved with Validation Loss: 1522478.0125\n",
      "Model saved with Validation Loss: 1521654.0750\n",
      "Model saved with Validation Loss: 1501585.1500\n",
      "Model saved with Validation Loss: 1492060.7875\n",
      "Model saved with Validation Loss: 1480026.2000\n",
      "Model saved with Validation Loss: 1458369.5875\n",
      "Model saved with Validation Loss: 1455748.5625\n",
      "Model saved with Validation Loss: 1445385.8000\n",
      "Model saved with Validation Loss: 1442011.4750\n",
      "Model saved with Validation Loss: 1436921.5125\n",
      "Epoch 50/50000, Training Loss: 1431686.2972\n",
      "Epoch 50/50000, Validation Loss: 1432480.6875\n",
      "Model saved with Validation Loss: 1432480.6875\n",
      "Model saved with Validation Loss: 1422843.9125\n",
      "Model saved with Validation Loss: 1413101.2125\n",
      "Model saved with Validation Loss: 1411151.4125\n",
      "Model saved with Validation Loss: 1404989.3250\n",
      "Model saved with Validation Loss: 1402341.4125\n",
      "Model saved with Validation Loss: 1401907.3500\n",
      "Model saved with Validation Loss: 1390579.7375\n",
      "Model saved with Validation Loss: 1389393.4000\n",
      "Model saved with Validation Loss: 1377629.8125\n",
      "Model saved with Validation Loss: 1363450.6250\n",
      "Model saved with Validation Loss: 1353102.4875\n",
      "Model saved with Validation Loss: 1341929.5375\n",
      "Model saved with Validation Loss: 1320645.3875\n",
      "Model saved with Validation Loss: 1317997.4000\n",
      "Model saved with Validation Loss: 1313436.9125\n",
      "Model saved with Validation Loss: 1303867.5625\n",
      "Model saved with Validation Loss: 1285951.9750\n",
      "Epoch 100/50000, Training Loss: 1278047.7193\n",
      "Epoch 100/50000, Validation Loss: 1295664.8125\n",
      "Model saved with Validation Loss: 1284195.7875\n",
      "Model saved with Validation Loss: 1282756.8625\n",
      "Model saved with Validation Loss: 1266501.9250\n",
      "Model saved with Validation Loss: 1244993.8750\n",
      "Model saved with Validation Loss: 1227341.0250\n",
      "Model saved with Validation Loss: 1217190.3250\n",
      "Model saved with Validation Loss: 1213781.5625\n",
      "Model saved with Validation Loss: 1196292.0250\n",
      "Model saved with Validation Loss: 1188536.3625\n",
      "Model saved with Validation Loss: 1153579.7500\n",
      "Model saved with Validation Loss: 1147319.4250\n",
      "Model saved with Validation Loss: 1132141.8750\n",
      "Epoch 150/50000, Training Loss: 1153034.5731\n",
      "Epoch 150/50000, Validation Loss: 1133195.6500\n",
      "Model saved with Validation Loss: 1112954.4000\n",
      "Model saved with Validation Loss: 1097214.8375\n",
      "Model saved with Validation Loss: 1080624.0500\n",
      "Model saved with Validation Loss: 1052376.9688\n",
      "Model saved with Validation Loss: 1047837.9938\n",
      "Model saved with Validation Loss: 1021914.9187\n",
      "Model saved with Validation Loss: 1016476.3688\n",
      "Epoch 200/50000, Training Loss: 1020646.5542\n",
      "Epoch 200/50000, Validation Loss: 1030534.1375\n",
      "Model saved with Validation Loss: 1005768.0000\n",
      "Model saved with Validation Loss: 1003173.1687\n",
      "Model saved with Validation Loss: 1000713.8063\n",
      "Model saved with Validation Loss: 976385.7375\n",
      "Model saved with Validation Loss: 968165.0687\n",
      "Model saved with Validation Loss: 959074.2188\n",
      "Model saved with Validation Loss: 956070.4812\n",
      "Model saved with Validation Loss: 942958.5188\n",
      "Model saved with Validation Loss: 929886.3875\n",
      "Model saved with Validation Loss: 914215.3250\n",
      "Epoch 250/50000, Training Loss: 896829.6769\n",
      "Epoch 250/50000, Validation Loss: 922024.5687\n",
      "Model saved with Validation Loss: 901230.5125\n",
      "Model saved with Validation Loss: 883730.0188\n",
      "Model saved with Validation Loss: 879888.0625\n",
      "Model saved with Validation Loss: 862374.1687\n",
      "Model saved with Validation Loss: 839525.5813\n",
      "Model saved with Validation Loss: 837561.1438\n",
      "Model saved with Validation Loss: 836867.7688\n",
      "Model saved with Validation Loss: 798948.2063\n",
      "Model saved with Validation Loss: 787970.2375\n",
      "Model saved with Validation Loss: 787053.7438\n",
      "Epoch 300/50000, Training Loss: 785848.7111\n",
      "Epoch 300/50000, Validation Loss: 778645.4688\n",
      "Model saved with Validation Loss: 778645.4688\n",
      "Model saved with Validation Loss: 771728.4437\n",
      "Model saved with Validation Loss: 743493.9313\n",
      "Model saved with Validation Loss: 731953.7250\n",
      "Model saved with Validation Loss: 717985.9812\n",
      "Model saved with Validation Loss: 712105.1937\n",
      "Model saved with Validation Loss: 704625.1500\n",
      "Model saved with Validation Loss: 699415.5875\n",
      "Model saved with Validation Loss: 684460.5750\n",
      "Epoch 350/50000, Training Loss: 684551.9988\n",
      "Epoch 350/50000, Validation Loss: 681988.5938\n",
      "Model saved with Validation Loss: 681988.5938\n",
      "Model saved with Validation Loss: 676629.9812\n",
      "Model saved with Validation Loss: 674407.3313\n",
      "Model saved with Validation Loss: 654376.1188\n",
      "Model saved with Validation Loss: 648149.5563\n",
      "Model saved with Validation Loss: 642271.0500\n",
      "Model saved with Validation Loss: 641989.3063\n",
      "Model saved with Validation Loss: 625463.2875\n",
      "Model saved with Validation Loss: 624911.6438\n",
      "Model saved with Validation Loss: 602763.2438\n",
      "Model saved with Validation Loss: 601056.9000\n",
      "Model saved with Validation Loss: 580486.0062\n",
      "Model saved with Validation Loss: 577863.5625\n",
      "Epoch 400/50000, Training Loss: 590371.3892\n",
      "Epoch 400/50000, Validation Loss: 573277.0062\n",
      "Model saved with Validation Loss: 573277.0062\n",
      "Model saved with Validation Loss: 564252.7375\n",
      "Model saved with Validation Loss: 556815.8750\n",
      "Model saved with Validation Loss: 556620.9062\n",
      "Model saved with Validation Loss: 553856.2375\n",
      "Model saved with Validation Loss: 551553.0750\n",
      "Model saved with Validation Loss: 532011.5125\n",
      "Model saved with Validation Loss: 527119.6469\n",
      "Model saved with Validation Loss: 506259.1094\n",
      "Model saved with Validation Loss: 503047.7375\n",
      "Model saved with Validation Loss: 502527.6688\n",
      "Model saved with Validation Loss: 493158.2375\n",
      "Model saved with Validation Loss: 482497.5031\n",
      "Epoch 450/50000, Training Loss: 503746.9322\n",
      "Epoch 450/50000, Validation Loss: 488482.8875\n",
      "Model saved with Validation Loss: 478481.5344\n",
      "Model saved with Validation Loss: 477984.8500\n",
      "Model saved with Validation Loss: 475004.2313\n",
      "Model saved with Validation Loss: 473764.6406\n",
      "Model saved with Validation Loss: 457371.5531\n",
      "Model saved with Validation Loss: 456630.6625\n",
      "Model saved with Validation Loss: 452751.6906\n",
      "Model saved with Validation Loss: 451229.6125\n",
      "Model saved with Validation Loss: 440594.1844\n",
      "Model saved with Validation Loss: 429511.6937\n",
      "Model saved with Validation Loss: 422899.0469\n",
      "Model saved with Validation Loss: 412370.2094\n",
      "Epoch 500/50000, Training Loss: 423411.3213\n",
      "Epoch 500/50000, Validation Loss: 399665.6344\n",
      "Model saved with Validation Loss: 399665.6344\n",
      "Model saved with Validation Loss: 396843.9844\n",
      "Model saved with Validation Loss: 387610.0312\n",
      "Model saved with Validation Loss: 386537.8375\n",
      "Model saved with Validation Loss: 368351.0344\n",
      "Model saved with Validation Loss: 365935.5094\n",
      "Model saved with Validation Loss: 365883.7375\n",
      "Model saved with Validation Loss: 365467.5438\n",
      "Model saved with Validation Loss: 358557.6281\n",
      "Model saved with Validation Loss: 353741.9437\n",
      "Model saved with Validation Loss: 351970.4875\n",
      "Model saved with Validation Loss: 346786.3625\n",
      "Model saved with Validation Loss: 341524.0156\n",
      "Epoch 550/50000, Training Loss: 350254.1798\n",
      "Epoch 550/50000, Validation Loss: 352933.0250\n",
      "Model saved with Validation Loss: 328458.8625\n",
      "Model saved with Validation Loss: 327358.1500\n",
      "Model saved with Validation Loss: 326024.2500\n",
      "Model saved with Validation Loss: 319029.9813\n",
      "Model saved with Validation Loss: 316105.9656\n",
      "Model saved with Validation Loss: 315522.3219\n",
      "Model saved with Validation Loss: 311519.4750\n",
      "Model saved with Validation Loss: 309568.7719\n",
      "Model saved with Validation Loss: 306976.7250\n",
      "Model saved with Validation Loss: 302212.5125\n",
      "Model saved with Validation Loss: 299985.3500\n",
      "Model saved with Validation Loss: 281962.1531\n",
      "Epoch 600/50000, Training Loss: 285134.0731\n",
      "Epoch 600/50000, Validation Loss: 302216.6562\n",
      "Model saved with Validation Loss: 281124.4188\n",
      "Model saved with Validation Loss: 271689.7656\n",
      "Model saved with Validation Loss: 256838.8172\n",
      "Model saved with Validation Loss: 250649.2500\n",
      "Model saved with Validation Loss: 246918.4375\n",
      "Model saved with Validation Loss: 244224.8609\n",
      "Model saved with Validation Loss: 239190.1047\n",
      "Model saved with Validation Loss: 228572.2313\n",
      "Model saved with Validation Loss: 228278.2219\n",
      "Model saved with Validation Loss: 227599.3875\n",
      "Epoch 650/50000, Training Loss: 227694.3514\n",
      "Epoch 650/50000, Validation Loss: 236942.5797\n",
      "Model saved with Validation Loss: 221269.9625\n",
      "Model saved with Validation Loss: 215356.4125\n",
      "Model saved with Validation Loss: 211708.4641\n",
      "Model saved with Validation Loss: 208902.6187\n",
      "Model saved with Validation Loss: 206143.4141\n",
      "Model saved with Validation Loss: 202874.9016\n",
      "Model saved with Validation Loss: 195919.7219\n",
      "Model saved with Validation Loss: 187677.3734\n",
      "Model saved with Validation Loss: 180182.5250\n",
      "Model saved with Validation Loss: 175111.1984\n",
      "Model saved with Validation Loss: 172089.3469\n",
      "Model saved with Validation Loss: 170393.9031\n",
      "Epoch 700/50000, Training Loss: 176981.8974\n",
      "Epoch 700/50000, Validation Loss: 169950.1562\n",
      "Model saved with Validation Loss: 169950.1562\n",
      "Model saved with Validation Loss: 163458.8188\n",
      "Model saved with Validation Loss: 158215.4891\n",
      "Model saved with Validation Loss: 156560.7031\n",
      "Model saved with Validation Loss: 156326.8672\n",
      "Model saved with Validation Loss: 154367.4422\n",
      "Model saved with Validation Loss: 150557.3609\n",
      "Model saved with Validation Loss: 149820.2297\n",
      "Model saved with Validation Loss: 142911.7672\n",
      "Model saved with Validation Loss: 139912.9813\n",
      "Model saved with Validation Loss: 136608.7344\n",
      "Model saved with Validation Loss: 133775.4547\n",
      "Model saved with Validation Loss: 133112.3000\n",
      "Model saved with Validation Loss: 132430.4891\n",
      "Epoch 750/50000, Training Loss: 133326.9301\n",
      "Epoch 750/50000, Validation Loss: 140491.1250\n",
      "Model saved with Validation Loss: 129489.8375\n",
      "Model saved with Validation Loss: 128328.6797\n",
      "Model saved with Validation Loss: 124691.0320\n",
      "Model saved with Validation Loss: 122108.0086\n",
      "Model saved with Validation Loss: 119622.3930\n",
      "Model saved with Validation Loss: 116226.4398\n",
      "Model saved with Validation Loss: 114444.3844\n",
      "Model saved with Validation Loss: 108307.1898\n",
      "Model saved with Validation Loss: 106699.6094\n",
      "Model saved with Validation Loss: 101445.1062\n",
      "Model saved with Validation Loss: 100912.7625\n",
      "Model saved with Validation Loss: 98417.7289\n",
      "Model saved with Validation Loss: 92774.1836\n",
      "Model saved with Validation Loss: 88830.9859\n",
      "Epoch 800/50000, Training Loss: 98023.1962\n",
      "Epoch 800/50000, Validation Loss: 93632.9727\n",
      "Model saved with Validation Loss: 84353.3281\n",
      "Model saved with Validation Loss: 81143.3203\n",
      "Model saved with Validation Loss: 79064.7898\n",
      "Model saved with Validation Loss: 78998.7906\n",
      "Model saved with Validation Loss: 72886.2664\n",
      "Model saved with Validation Loss: 71615.3773\n",
      "Model saved with Validation Loss: 70945.0508\n",
      "Model saved with Validation Loss: 70669.2602\n",
      "Model saved with Validation Loss: 68509.0328\n",
      "Model saved with Validation Loss: 62119.0598\n",
      "Model saved with Validation Loss: 61089.4262\n",
      "Epoch 850/50000, Training Loss: 68237.1873\n",
      "Epoch 850/50000, Validation Loss: 60796.9543\n",
      "Model saved with Validation Loss: 60796.9543\n",
      "Model saved with Validation Loss: 58864.5609\n",
      "Model saved with Validation Loss: 58513.6082\n",
      "Model saved with Validation Loss: 58429.8832\n",
      "Model saved with Validation Loss: 54905.3242\n",
      "Model saved with Validation Loss: 52849.8824\n",
      "Model saved with Validation Loss: 50223.5734\n",
      "Model saved with Validation Loss: 50091.9922\n",
      "Model saved with Validation Loss: 45572.3500\n",
      "Model saved with Validation Loss: 41958.1777\n",
      "Model saved with Validation Loss: 39128.3961\n",
      "Epoch 900/50000, Training Loss: 46228.5965\n",
      "Epoch 900/50000, Validation Loss: 38062.7582\n",
      "Model saved with Validation Loss: 38062.7582\n",
      "Model saved with Validation Loss: 38050.4449\n",
      "Model saved with Validation Loss: 35999.1594\n",
      "Model saved with Validation Loss: 35375.9109\n",
      "Model saved with Validation Loss: 34571.3090\n",
      "Model saved with Validation Loss: 33434.3035\n",
      "Model saved with Validation Loss: 32018.3336\n",
      "Model saved with Validation Loss: 28742.0273\n",
      "Model saved with Validation Loss: 27769.8236\n",
      "Model saved with Validation Loss: 26946.4705\n",
      "Model saved with Validation Loss: 23585.7184\n",
      "Model saved with Validation Loss: 21589.5340\n",
      "Epoch 950/50000, Training Loss: 30264.9256\n",
      "Epoch 950/50000, Validation Loss: 22161.9486\n",
      "Model saved with Validation Loss: 20734.4545\n",
      "Model saved with Validation Loss: 18658.9449\n",
      "Model saved with Validation Loss: 17785.3359\n",
      "Model saved with Validation Loss: 17114.6232\n",
      "Model saved with Validation Loss: 15932.4354\n",
      "Model saved with Validation Loss: 15775.8582\n",
      "Model saved with Validation Loss: 13931.3531\n",
      "Model saved with Validation Loss: 13392.8455\n",
      "Model saved with Validation Loss: 12648.6086\n",
      "Model saved with Validation Loss: 11257.7799\n",
      "Model saved with Validation Loss: 10624.3825\n",
      "Epoch 1000/50000, Training Loss: 19948.6412\n",
      "Epoch 1000/50000, Validation Loss: 9949.5732\n",
      "Model saved with Validation Loss: 9949.5732\n",
      "Model saved with Validation Loss: 9876.7098\n",
      "Model saved with Validation Loss: 9410.3131\n",
      "Model saved with Validation Loss: 8937.2141\n",
      "Model saved with Validation Loss: 7462.5500\n",
      "Model saved with Validation Loss: 7400.9633\n",
      "Model saved with Validation Loss: 6077.0362\n",
      "Model saved with Validation Loss: 5746.7659\n",
      "Model saved with Validation Loss: 4681.4097\n",
      "Model saved with Validation Loss: 3922.8126\n",
      "Epoch 1050/50000, Training Loss: 14834.1156\n",
      "Epoch 1050/50000, Validation Loss: 5837.8127\n",
      "Model saved with Validation Loss: 3795.9621\n",
      "Model saved with Validation Loss: 3515.4339\n",
      "Model saved with Validation Loss: 3205.5815\n",
      "Model saved with Validation Loss: 2912.8814\n",
      "Model saved with Validation Loss: 2768.5661\n",
      "Model saved with Validation Loss: 2382.0858\n",
      "Model saved with Validation Loss: 2322.8133\n",
      "Epoch 1100/50000, Training Loss: 12347.9342\n",
      "Epoch 1100/50000, Validation Loss: 2565.9237\n",
      "Model saved with Validation Loss: 2095.8559\n",
      "Model saved with Validation Loss: 1929.3307\n",
      "Epoch 1150/50000, Training Loss: 11397.5217\n",
      "Epoch 1150/50000, Validation Loss: 2960.4760\n",
      "Model saved with Validation Loss: 1883.2760\n",
      "Model saved with Validation Loss: 1857.5346\n",
      "Model saved with Validation Loss: 1635.7146\n",
      "Epoch 1200/50000, Training Loss: 10315.3553\n",
      "Epoch 1200/50000, Validation Loss: 2254.3468\n",
      "Epoch 1250/50000, Training Loss: 9727.3863\n",
      "Epoch 1250/50000, Validation Loss: 1826.5575\n",
      "Model saved with Validation Loss: 1628.7912\n",
      "Model saved with Validation Loss: 1578.4848\n",
      "Model saved with Validation Loss: 1492.3523\n",
      "Model saved with Validation Loss: 1438.9946\n",
      "Epoch 1300/50000, Training Loss: 9286.2147\n",
      "Epoch 1300/50000, Validation Loss: 1839.9511\n",
      "Epoch 1350/50000, Training Loss: 8506.2324\n",
      "Epoch 1350/50000, Validation Loss: 1434.5017\n",
      "Model saved with Validation Loss: 1434.5017\n",
      "Model saved with Validation Loss: 1342.0057\n",
      "Model saved with Validation Loss: 1251.5624\n",
      "Model saved with Validation Loss: 1212.6835\n",
      "Epoch 1400/50000, Training Loss: 8188.6996\n",
      "Epoch 1400/50000, Validation Loss: 1427.4072\n",
      "Epoch 1450/50000, Training Loss: 8013.4053\n",
      "Epoch 1450/50000, Validation Loss: 1584.0472\n",
      "Model saved with Validation Loss: 1095.6268\n",
      "Epoch 1500/50000, Training Loss: 7663.6434\n",
      "Epoch 1500/50000, Validation Loss: 1400.0816\n",
      "Model saved with Validation Loss: 1073.7904\n",
      "Epoch 1550/50000, Training Loss: 7251.7174\n",
      "Epoch 1550/50000, Validation Loss: 1492.8028\n",
      "Model saved with Validation Loss: 1065.0547\n",
      "Epoch 1600/50000, Training Loss: 7050.9273\n",
      "Epoch 1600/50000, Validation Loss: 1190.0294\n",
      "Model saved with Validation Loss: 1053.2349\n",
      "Model saved with Validation Loss: 1005.7134\n",
      "Epoch 1650/50000, Training Loss: 7038.8976\n",
      "Epoch 1650/50000, Validation Loss: 1668.4483\n",
      "Model saved with Validation Loss: 950.7572\n",
      "Epoch 1700/50000, Training Loss: 6820.6540\n",
      "Epoch 1700/50000, Validation Loss: 1574.3262\n",
      "Model saved with Validation Loss: 930.8406\n",
      "Epoch 1750/50000, Training Loss: 6570.9143\n",
      "Epoch 1750/50000, Validation Loss: 1045.1954\n",
      "Epoch 1800/50000, Training Loss: 6559.4431\n",
      "Epoch 1800/50000, Validation Loss: 1086.9197\n",
      "Model saved with Validation Loss: 898.3496\n",
      "Model saved with Validation Loss: 864.7145\n",
      "Model saved with Validation Loss: 858.2079\n",
      "Epoch 1850/50000, Training Loss: 6489.9522\n",
      "Epoch 1850/50000, Validation Loss: 1049.4472\n",
      "Model saved with Validation Loss: 838.9636\n",
      "Epoch 1900/50000, Training Loss: 6225.9633\n",
      "Epoch 1900/50000, Validation Loss: 1045.0427\n",
      "Epoch 1950/50000, Training Loss: 6082.8850\n",
      "Epoch 1950/50000, Validation Loss: 1300.5171\n",
      "Epoch 2000/50000, Training Loss: 6071.1566\n",
      "Epoch 2000/50000, Validation Loss: 1212.3526\n",
      "Model saved with Validation Loss: 817.0187\n",
      "Epoch 2050/50000, Training Loss: 6010.6344\n",
      "Epoch 2050/50000, Validation Loss: 1204.0882\n",
      "Model saved with Validation Loss: 803.1267\n",
      "Model saved with Validation Loss: 777.2400\n",
      "Model saved with Validation Loss: 745.7085\n",
      "Epoch 2100/50000, Training Loss: 5821.2645\n",
      "Epoch 2100/50000, Validation Loss: 1037.4096\n",
      "Epoch 2150/50000, Training Loss: 5647.1660\n",
      "Epoch 2150/50000, Validation Loss: 1169.9879\n",
      "Epoch 2200/50000, Training Loss: 5812.0037\n",
      "Epoch 2200/50000, Validation Loss: 913.5739\n",
      "Model saved with Validation Loss: 732.4469\n",
      "Epoch 2250/50000, Training Loss: 5487.4390\n",
      "Epoch 2250/50000, Validation Loss: 1214.9044\n",
      "Epoch 2300/50000, Training Loss: 5750.3851\n",
      "Epoch 2300/50000, Validation Loss: 964.1694\n",
      "Model saved with Validation Loss: 731.5931\n",
      "Epoch 2350/50000, Training Loss: 5497.3772\n",
      "Epoch 2350/50000, Validation Loss: 1210.5051\n",
      "Epoch 2400/50000, Training Loss: 5529.4862\n",
      "Epoch 2400/50000, Validation Loss: 916.9630\n",
      "Model saved with Validation Loss: 702.9400\n",
      "Epoch 2450/50000, Training Loss: 5466.8691\n",
      "Epoch 2450/50000, Validation Loss: 1208.1497\n",
      "Model saved with Validation Loss: 695.5369\n",
      "Epoch 2500/50000, Training Loss: 5378.5562\n",
      "Epoch 2500/50000, Validation Loss: 1007.6642\n",
      "Model saved with Validation Loss: 644.9690\n",
      "Epoch 2550/50000, Training Loss: 5203.5765\n",
      "Epoch 2550/50000, Validation Loss: 1012.7674\n",
      "Epoch 2600/50000, Training Loss: 5241.5545\n",
      "Epoch 2600/50000, Validation Loss: 817.7331\n",
      "Epoch 2650/50000, Training Loss: 5158.7436\n",
      "Epoch 2650/50000, Validation Loss: 1010.9929\n",
      "Epoch 2700/50000, Training Loss: 5066.0947\n",
      "Epoch 2700/50000, Validation Loss: 684.6431\n",
      "Model saved with Validation Loss: 636.0284\n",
      "Model saved with Validation Loss: 633.4257\n",
      "Epoch 2750/50000, Training Loss: 5038.5888\n",
      "Epoch 2750/50000, Validation Loss: 824.5360\n",
      "Epoch 2800/50000, Training Loss: 5049.6655\n",
      "Epoch 2800/50000, Validation Loss: 988.8034\n",
      "Epoch 2850/50000, Training Loss: 4941.4375\n",
      "Epoch 2850/50000, Validation Loss: 1092.3746\n",
      "Epoch 2900/50000, Training Loss: 4859.5576\n",
      "Epoch 2900/50000, Validation Loss: 686.8743\n",
      "Epoch 2950/50000, Training Loss: 4878.3726\n",
      "Epoch 2950/50000, Validation Loss: 835.1372\n",
      "Model saved with Validation Loss: 617.4131\n",
      "Epoch 3000/50000, Training Loss: 4695.2875\n",
      "Epoch 3000/50000, Validation Loss: 783.3986\n",
      "Epoch 3050/50000, Training Loss: 4891.7720\n",
      "Epoch 3050/50000, Validation Loss: 647.7008\n",
      "Model saved with Validation Loss: 612.4674\n",
      "Epoch 3100/50000, Training Loss: 4726.5021\n",
      "Epoch 3100/50000, Validation Loss: 794.3594\n",
      "Model saved with Validation Loss: 588.7862\n",
      "Epoch 3150/50000, Training Loss: 4837.2926\n",
      "Epoch 3150/50000, Validation Loss: 664.9777\n",
      "Epoch 3200/50000, Training Loss: 4679.6415\n",
      "Epoch 3200/50000, Validation Loss: 637.7859\n",
      "Model saved with Validation Loss: 554.8174\n",
      "Epoch 3250/50000, Training Loss: 4737.6899\n",
      "Epoch 3250/50000, Validation Loss: 882.0328\n",
      "Epoch 3300/50000, Training Loss: 4557.5394\n",
      "Epoch 3300/50000, Validation Loss: 735.9030\n",
      "Epoch 3350/50000, Training Loss: 4616.6708\n",
      "Epoch 3350/50000, Validation Loss: 635.5997\n",
      "Epoch 3400/50000, Training Loss: 4547.3210\n",
      "Epoch 3400/50000, Validation Loss: 654.1441\n",
      "Epoch 3450/50000, Training Loss: 4508.5980\n",
      "Epoch 3450/50000, Validation Loss: 840.6807\n",
      "Epoch 3500/50000, Training Loss: 4630.0298\n",
      "Epoch 3500/50000, Validation Loss: 828.9678\n",
      "Epoch 3550/50000, Training Loss: 4384.9215\n",
      "Epoch 3550/50000, Validation Loss: 768.1313\n",
      "Model saved with Validation Loss: 547.6438\n",
      "Epoch 3600/50000, Training Loss: 4483.8446\n",
      "Epoch 3600/50000, Validation Loss: 628.0557\n",
      "Model saved with Validation Loss: 531.9934\n",
      "Epoch 3650/50000, Training Loss: 4350.7910\n",
      "Epoch 3650/50000, Validation Loss: 685.7634\n",
      "Epoch 3700/50000, Training Loss: 4389.6560\n",
      "Epoch 3700/50000, Validation Loss: 609.5528\n",
      "Epoch 3750/50000, Training Loss: 4270.7293\n",
      "Epoch 3750/50000, Validation Loss: 531.6559\n",
      "Model saved with Validation Loss: 531.6559\n",
      "Epoch 3800/50000, Training Loss: 4340.2823\n",
      "Epoch 3800/50000, Validation Loss: 648.2981\n",
      "Epoch 3850/50000, Training Loss: 4406.7455\n",
      "Epoch 3850/50000, Validation Loss: 521.5468\n",
      "Model saved with Validation Loss: 521.5468\n",
      "Epoch 3900/50000, Training Loss: 4249.8254\n",
      "Epoch 3900/50000, Validation Loss: 635.8024\n",
      "Epoch 3950/50000, Training Loss: 4294.0200\n",
      "Epoch 3950/50000, Validation Loss: 729.9685\n",
      "Model saved with Validation Loss: 519.3005\n",
      "Epoch 4000/50000, Training Loss: 4264.6270\n",
      "Epoch 4000/50000, Validation Loss: 673.0089\n",
      "Model saved with Validation Loss: 507.6649\n",
      "Epoch 4050/50000, Training Loss: 4195.4469\n",
      "Epoch 4050/50000, Validation Loss: 757.9355\n",
      "Epoch 4100/50000, Training Loss: 4244.8039\n",
      "Epoch 4100/50000, Validation Loss: 659.1401\n",
      "Model saved with Validation Loss: 506.5496\n",
      "Model saved with Validation Loss: 472.6631\n",
      "Epoch 4150/50000, Training Loss: 4276.8673\n",
      "Epoch 4150/50000, Validation Loss: 639.3958\n",
      "Epoch 4200/50000, Training Loss: 4174.6805\n",
      "Epoch 4200/50000, Validation Loss: 742.9884\n",
      "Epoch 4250/50000, Training Loss: 4222.6130\n",
      "Epoch 4250/50000, Validation Loss: 606.8507\n",
      "Epoch 4300/50000, Training Loss: 4132.5370\n",
      "Epoch 4300/50000, Validation Loss: 657.9086\n",
      "Epoch 4350/50000, Training Loss: 4144.8528\n",
      "Epoch 4350/50000, Validation Loss: 615.6404\n",
      "Epoch 4400/50000, Training Loss: 4081.6912\n",
      "Epoch 4400/50000, Validation Loss: 532.0569\n",
      "Epoch 4450/50000, Training Loss: 4051.4415\n",
      "Epoch 4450/50000, Validation Loss: 698.6632\n",
      "Model saved with Validation Loss: 443.0814\n",
      "Epoch 4500/50000, Training Loss: 4193.1786\n",
      "Epoch 4500/50000, Validation Loss: 645.6936\n",
      "Epoch 4550/50000, Training Loss: 4102.4985\n",
      "Epoch 4550/50000, Validation Loss: 538.8137\n",
      "Epoch 4600/50000, Training Loss: 4080.6935\n",
      "Epoch 4600/50000, Validation Loss: 591.2014\n",
      "Epoch 4650/50000, Training Loss: 4040.4765\n",
      "Epoch 4650/50000, Validation Loss: 501.1205\n",
      "Epoch 4700/50000, Training Loss: 3995.7284\n",
      "Epoch 4700/50000, Validation Loss: 529.7771\n",
      "Epoch 4750/50000, Training Loss: 3982.6161\n",
      "Epoch 4750/50000, Validation Loss: 566.9828\n",
      "Epoch 4800/50000, Training Loss: 3986.2997\n",
      "Epoch 4800/50000, Validation Loss: 633.1534\n",
      "Model saved with Validation Loss: 441.9010\n",
      "Epoch 4850/50000, Training Loss: 4010.0713\n",
      "Epoch 4850/50000, Validation Loss: 633.9860\n",
      "Model saved with Validation Loss: 435.5308\n",
      "Epoch 4900/50000, Training Loss: 3852.1708\n",
      "Epoch 4900/50000, Validation Loss: 551.3521\n",
      "Epoch 4950/50000, Training Loss: 3932.2545\n",
      "Epoch 4950/50000, Validation Loss: 600.4669\n",
      "Epoch 5000/50000, Training Loss: 3940.7074\n",
      "Epoch 5000/50000, Validation Loss: 499.2464\n",
      "Model saved with Validation Loss: 429.2386\n",
      "Epoch 5050/50000, Training Loss: 3881.4087\n",
      "Epoch 5050/50000, Validation Loss: 562.0179\n",
      "Epoch 5100/50000, Training Loss: 3824.4675\n",
      "Epoch 5100/50000, Validation Loss: 586.1811\n",
      "Epoch 5150/50000, Training Loss: 3925.7724\n",
      "Epoch 5150/50000, Validation Loss: 598.5375\n",
      "Model saved with Validation Loss: 428.6613\n",
      "Epoch 5200/50000, Training Loss: 3762.2385\n",
      "Epoch 5200/50000, Validation Loss: 516.5888\n",
      "Epoch 5250/50000, Training Loss: 3754.9442\n",
      "Epoch 5250/50000, Validation Loss: 599.5514\n",
      "Model saved with Validation Loss: 421.0679\n",
      "Epoch 5300/50000, Training Loss: 3774.0304\n",
      "Epoch 5300/50000, Validation Loss: 427.1730\n",
      "Epoch 5350/50000, Training Loss: 3855.2300\n",
      "Epoch 5350/50000, Validation Loss: 585.0002\n",
      "Model saved with Validation Loss: 415.2136\n",
      "Epoch 5400/50000, Training Loss: 3854.5050\n",
      "Epoch 5400/50000, Validation Loss: 449.0651\n",
      "Epoch 5450/50000, Training Loss: 3870.6021\n",
      "Epoch 5450/50000, Validation Loss: 487.1517\n",
      "Model saved with Validation Loss: 412.1937\n",
      "Epoch 5500/50000, Training Loss: 3787.9995\n",
      "Epoch 5500/50000, Validation Loss: 454.0804\n",
      "Epoch 5550/50000, Training Loss: 3765.0088\n",
      "Epoch 5550/50000, Validation Loss: 495.7380\n",
      "Epoch 5600/50000, Training Loss: 3848.5406\n",
      "Epoch 5600/50000, Validation Loss: 440.3898\n",
      "Epoch 5650/50000, Training Loss: 3703.5414\n",
      "Epoch 5650/50000, Validation Loss: 465.1021\n",
      "Epoch 5700/50000, Training Loss: 3792.0812\n",
      "Epoch 5700/50000, Validation Loss: 511.5627\n",
      "Model saved with Validation Loss: 399.7345\n",
      "Model saved with Validation Loss: 397.8410\n",
      "Epoch 5750/50000, Training Loss: 3645.5153\n",
      "Epoch 5750/50000, Validation Loss: 420.0253\n",
      "Epoch 5800/50000, Training Loss: 3727.8148\n",
      "Epoch 5800/50000, Validation Loss: 475.1793\n",
      "Epoch 5850/50000, Training Loss: 3701.7105\n",
      "Epoch 5850/50000, Validation Loss: 484.6853\n",
      "Epoch 5900/50000, Training Loss: 3684.1673\n",
      "Epoch 5900/50000, Validation Loss: 541.1135\n",
      "Epoch 5950/50000, Training Loss: 3540.2943\n",
      "Epoch 5950/50000, Validation Loss: 566.2750\n",
      "Epoch 6000/50000, Training Loss: 3616.3020\n",
      "Epoch 6000/50000, Validation Loss: 525.4505\n",
      "Epoch 6050/50000, Training Loss: 3559.4794\n",
      "Epoch 6050/50000, Validation Loss: 521.9269\n",
      "Epoch 6100/50000, Training Loss: 3600.2964\n",
      "Epoch 6100/50000, Validation Loss: 542.4729\n",
      "Epoch 6150/50000, Training Loss: 3633.7913\n",
      "Epoch 6150/50000, Validation Loss: 630.9828\n",
      "Model saved with Validation Loss: 396.4752\n",
      "Epoch 6200/50000, Training Loss: 3489.7828\n",
      "Epoch 6200/50000, Validation Loss: 517.8733\n",
      "Model saved with Validation Loss: 378.6636\n",
      "Epoch 6250/50000, Training Loss: 3637.1821\n",
      "Epoch 6250/50000, Validation Loss: 543.4513\n",
      "Epoch 6300/50000, Training Loss: 3584.4084\n",
      "Epoch 6300/50000, Validation Loss: 480.4181\n",
      "Model saved with Validation Loss: 376.1242\n",
      "Epoch 6350/50000, Training Loss: 3549.1765\n",
      "Epoch 6350/50000, Validation Loss: 498.1805\n",
      "Epoch 6400/50000, Training Loss: 3632.8303\n",
      "Epoch 6400/50000, Validation Loss: 405.2357\n",
      "Epoch 6450/50000, Training Loss: 3555.5624\n",
      "Epoch 6450/50000, Validation Loss: 443.8105\n",
      "Epoch 6500/50000, Training Loss: 3562.2015\n",
      "Epoch 6500/50000, Validation Loss: 681.1601\n",
      "Model saved with Validation Loss: 369.5273\n",
      "Epoch 6550/50000, Training Loss: 3578.1205\n",
      "Epoch 6550/50000, Validation Loss: 511.8488\n",
      "Epoch 6600/50000, Training Loss: 3473.1634\n",
      "Epoch 6600/50000, Validation Loss: 467.6347\n",
      "Epoch 6650/50000, Training Loss: 3478.4862\n",
      "Epoch 6650/50000, Validation Loss: 535.3852\n",
      "Epoch 6700/50000, Training Loss: 3570.5460\n",
      "Epoch 6700/50000, Validation Loss: 389.4181\n",
      "Epoch 6750/50000, Training Loss: 3420.8439\n",
      "Epoch 6750/50000, Validation Loss: 461.7443\n",
      "Epoch 6800/50000, Training Loss: 3568.7280\n",
      "Epoch 6800/50000, Validation Loss: 570.8807\n",
      "Epoch 6850/50000, Training Loss: 3569.9146\n",
      "Epoch 6850/50000, Validation Loss: 577.5334\n",
      "Epoch 6900/50000, Training Loss: 3546.0315\n",
      "Epoch 6900/50000, Validation Loss: 400.4842\n",
      "Model saved with Validation Loss: 361.9022\n",
      "Model saved with Validation Loss: 356.7977\n",
      "Epoch 6950/50000, Training Loss: 3403.5366\n",
      "Epoch 6950/50000, Validation Loss: 410.1204\n",
      "Epoch 7000/50000, Training Loss: 3576.9578\n",
      "Epoch 7000/50000, Validation Loss: 475.3429\n",
      "Epoch 7050/50000, Training Loss: 3422.4638\n",
      "Epoch 7050/50000, Validation Loss: 535.1939\n",
      "Epoch 7100/50000, Training Loss: 3418.6975\n",
      "Epoch 7100/50000, Validation Loss: 522.5120\n",
      "Epoch 7150/50000, Training Loss: 3412.6774\n",
      "Epoch 7150/50000, Validation Loss: 457.0167\n",
      "Epoch 7200/50000, Training Loss: 3484.7578\n",
      "Epoch 7200/50000, Validation Loss: 541.1632\n",
      "Epoch 7250/50000, Training Loss: 3431.9614\n",
      "Epoch 7250/50000, Validation Loss: 420.6513\n",
      "Model saved with Validation Loss: 346.2914\n",
      "Epoch 7300/50000, Training Loss: 3453.1060\n",
      "Epoch 7300/50000, Validation Loss: 494.4911\n",
      "Epoch 7350/50000, Training Loss: 3442.5977\n",
      "Epoch 7350/50000, Validation Loss: 423.4795\n",
      "Epoch 7400/50000, Training Loss: 3371.6734\n",
      "Epoch 7400/50000, Validation Loss: 378.5097\n",
      "Epoch 7450/50000, Training Loss: 3482.8655\n",
      "Epoch 7450/50000, Validation Loss: 417.9436\n",
      "Epoch 7500/50000, Training Loss: 3485.9935\n",
      "Epoch 7500/50000, Validation Loss: 404.4360\n",
      "Epoch 7550/50000, Training Loss: 3370.5556\n",
      "Epoch 7550/50000, Validation Loss: 440.7999\n",
      "Model saved with Validation Loss: 322.0944\n",
      "Epoch 7600/50000, Training Loss: 3447.9183\n",
      "Epoch 7600/50000, Validation Loss: 462.6751\n",
      "Epoch 7650/50000, Training Loss: 3474.7658\n",
      "Epoch 7650/50000, Validation Loss: 484.1990\n",
      "Epoch 7700/50000, Training Loss: 3378.6278\n",
      "Epoch 7700/50000, Validation Loss: 533.2434\n",
      "Epoch 7750/50000, Training Loss: 3394.3610\n",
      "Epoch 7750/50000, Validation Loss: 379.2089\n",
      "Epoch 7800/50000, Training Loss: 3391.5766\n",
      "Epoch 7800/50000, Validation Loss: 388.2261\n",
      "Epoch 7850/50000, Training Loss: 3391.7800\n",
      "Epoch 7850/50000, Validation Loss: 367.6013\n",
      "Epoch 7900/50000, Training Loss: 3299.6926\n",
      "Epoch 7900/50000, Validation Loss: 461.1031\n",
      "Epoch 7950/50000, Training Loss: 3393.2477\n",
      "Epoch 7950/50000, Validation Loss: 336.9901\n",
      "Epoch 8000/50000, Training Loss: 3454.8310\n",
      "Epoch 8000/50000, Validation Loss: 474.1565\n",
      "Epoch 8050/50000, Training Loss: 3347.3917\n",
      "Epoch 8050/50000, Validation Loss: 413.2238\n",
      "Epoch 8100/50000, Training Loss: 3304.1017\n",
      "Epoch 8100/50000, Validation Loss: 476.2468\n",
      "Epoch 8150/50000, Training Loss: 3365.4869\n",
      "Epoch 8150/50000, Validation Loss: 508.4353\n",
      "Epoch 8200/50000, Training Loss: 3432.0734\n",
      "Epoch 8200/50000, Validation Loss: 655.4368\n",
      "Epoch 8250/50000, Training Loss: 3246.6168\n",
      "Epoch 8250/50000, Validation Loss: 531.6766\n",
      "Epoch 8300/50000, Training Loss: 3341.3470\n",
      "Epoch 8300/50000, Validation Loss: 399.5367\n",
      "Epoch 8350/50000, Training Loss: 3235.6105\n",
      "Epoch 8350/50000, Validation Loss: 454.9010\n",
      "Epoch 8400/50000, Training Loss: 3313.6998\n",
      "Epoch 8400/50000, Validation Loss: 440.1966\n",
      "Epoch 8450/50000, Training Loss: 3354.7421\n",
      "Epoch 8450/50000, Validation Loss: 486.0868\n",
      "Epoch 8500/50000, Training Loss: 3305.1452\n",
      "Epoch 8500/50000, Validation Loss: 495.2922\n",
      "Epoch 8550/50000, Training Loss: 3238.1280\n",
      "Epoch 8550/50000, Validation Loss: 397.6083\n",
      "Epoch 8600/50000, Training Loss: 3202.6494\n",
      "Epoch 8600/50000, Validation Loss: 353.8901\n",
      "Epoch 8650/50000, Training Loss: 3206.8070\n",
      "Epoch 8650/50000, Validation Loss: 368.1628\n",
      "Epoch 8700/50000, Training Loss: 3164.2575\n",
      "Epoch 8700/50000, Validation Loss: 444.0447\n",
      "Epoch 8750/50000, Training Loss: 3176.7984\n",
      "Epoch 8750/50000, Validation Loss: 316.8477\n",
      "Model saved with Validation Loss: 316.8477\n",
      "Epoch 8800/50000, Training Loss: 3164.6839\n",
      "Epoch 8800/50000, Validation Loss: 451.3282\n",
      "Epoch 8850/50000, Training Loss: 3189.3312\n",
      "Epoch 8850/50000, Validation Loss: 344.7875\n",
      "Epoch 8900/50000, Training Loss: 3139.1948\n",
      "Epoch 8900/50000, Validation Loss: 379.0604\n",
      "Epoch 8950/50000, Training Loss: 3140.7935\n",
      "Epoch 8950/50000, Validation Loss: 442.3691\n",
      "Epoch 9000/50000, Training Loss: 3098.1733\n",
      "Epoch 9000/50000, Validation Loss: 377.4452\n",
      "Model saved with Validation Loss: 303.2191\n",
      "Epoch 9050/50000, Training Loss: 3163.5536\n",
      "Epoch 9050/50000, Validation Loss: 472.9594\n",
      "Epoch 9100/50000, Training Loss: 3188.8617\n",
      "Epoch 9100/50000, Validation Loss: 459.7020\n",
      "Epoch 9150/50000, Training Loss: 3140.9929\n",
      "Epoch 9150/50000, Validation Loss: 432.4780\n",
      "Epoch 9200/50000, Training Loss: 3177.8295\n",
      "Epoch 9200/50000, Validation Loss: 493.2721\n",
      "Epoch 9250/50000, Training Loss: 3118.9023\n",
      "Epoch 9250/50000, Validation Loss: 446.8862\n",
      "Epoch 9300/50000, Training Loss: 3143.0964\n",
      "Epoch 9300/50000, Validation Loss: 388.5449\n",
      "Epoch 9350/50000, Training Loss: 3099.2909\n",
      "Epoch 9350/50000, Validation Loss: 383.6699\n",
      "Epoch 9400/50000, Training Loss: 3130.5632\n",
      "Epoch 9400/50000, Validation Loss: 426.9510\n",
      "Epoch 9450/50000, Training Loss: 3130.4605\n",
      "Epoch 9450/50000, Validation Loss: 402.5591\n",
      "Epoch 9500/50000, Training Loss: 3188.7529\n",
      "Epoch 9500/50000, Validation Loss: 398.6243\n",
      "Epoch 9550/50000, Training Loss: 3133.5212\n",
      "Epoch 9550/50000, Validation Loss: 405.9607\n",
      "Epoch 9600/50000, Training Loss: 3181.4087\n",
      "Epoch 9600/50000, Validation Loss: 356.0058\n",
      "Epoch 9650/50000, Training Loss: 3082.1971\n",
      "Epoch 9650/50000, Validation Loss: 371.1272\n",
      "Epoch 9700/50000, Training Loss: 3112.3931\n",
      "Epoch 9700/50000, Validation Loss: 408.3509\n",
      "Epoch 9750/50000, Training Loss: 3106.5213\n",
      "Epoch 9750/50000, Validation Loss: 415.8668\n",
      "Epoch 9800/50000, Training Loss: 3151.0605\n",
      "Epoch 9800/50000, Validation Loss: 395.1210\n",
      "Model saved with Validation Loss: 298.8113\n",
      "Epoch 9850/50000, Training Loss: 3123.7971\n",
      "Epoch 9850/50000, Validation Loss: 369.4290\n",
      "Epoch 9900/50000, Training Loss: 3013.8534\n",
      "Epoch 9900/50000, Validation Loss: 366.0554\n",
      "Epoch 9950/50000, Training Loss: 3137.2297\n",
      "Epoch 9950/50000, Validation Loss: 402.4239\n",
      "Epoch 10000/50000, Training Loss: 3085.4345\n",
      "Epoch 10000/50000, Validation Loss: 371.7636\n",
      "Epoch 10050/50000, Training Loss: 3092.6759\n",
      "Epoch 10050/50000, Validation Loss: 360.6249\n",
      "Epoch 10100/50000, Training Loss: 3140.4376\n",
      "Epoch 10100/50000, Validation Loss: 431.3976\n",
      "Epoch 10150/50000, Training Loss: 3072.0178\n",
      "Epoch 10150/50000, Validation Loss: 379.3929\n",
      "Epoch 10200/50000, Training Loss: 3212.9655\n",
      "Epoch 10200/50000, Validation Loss: 351.1611\n",
      "Epoch 10250/50000, Training Loss: 3054.1587\n",
      "Epoch 10250/50000, Validation Loss: 479.5021\n",
      "Epoch 10300/50000, Training Loss: 3117.5176\n",
      "Epoch 10300/50000, Validation Loss: 407.6608\n",
      "Epoch 10350/50000, Training Loss: 3145.4814\n",
      "Epoch 10350/50000, Validation Loss: 451.8209\n",
      "Epoch 10400/50000, Training Loss: 3071.4318\n",
      "Epoch 10400/50000, Validation Loss: 399.3835\n",
      "Epoch 10450/50000, Training Loss: 3091.3826\n",
      "Epoch 10450/50000, Validation Loss: 353.0461\n",
      "Epoch 10500/50000, Training Loss: 3053.4908\n",
      "Epoch 10500/50000, Validation Loss: 344.7480\n",
      "Epoch 10550/50000, Training Loss: 3099.8989\n",
      "Epoch 10550/50000, Validation Loss: 410.3948\n",
      "Epoch 10600/50000, Training Loss: 3061.2022\n",
      "Epoch 10600/50000, Validation Loss: 423.1695\n",
      "Epoch 10650/50000, Training Loss: 3039.9863\n",
      "Epoch 10650/50000, Validation Loss: 441.7398\n",
      "Epoch 10700/50000, Training Loss: 3127.1181\n",
      "Epoch 10700/50000, Validation Loss: 424.9830\n",
      "Epoch 10750/50000, Training Loss: 3061.7956\n",
      "Epoch 10750/50000, Validation Loss: 435.3334\n",
      "Epoch 10800/50000, Training Loss: 3126.4608\n",
      "Epoch 10800/50000, Validation Loss: 446.6058\n",
      "Model saved with Validation Loss: 297.4924\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#left_arm_idx = list(range(left_wrist_idx + 1, left_wrist_idx + num_left_arm_landmarks))\u001b[39;00m\n\u001b[1;32m      4\u001b[0m left_arm_idx \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m19\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m23\u001b[39m, \u001b[38;5;241m24\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_seq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_left_arm_hand_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_arm_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/dev/pose_sandbox/Hand_pose_estimation_3D/arm_and_hand/train_ann_no_intrinsics.py:76\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, num_epochs, save_path, early_stopping, scheduler, writer, log_seq, weight_idx, weight, train_left_arm_hand_only)\u001b[0m\n\u001b[1;32m     74\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape: (B, 144), B = batch_size, 144 = output_dim\u001b[39;00m\n\u001b[1;32m     77\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m48\u001b[39m)  \u001b[38;5;66;03m# shape: (B, 3, 44). For now, we use fused thumb as input => already removed thumb in output nodes\u001b[39;00m\n\u001b[1;32m     78\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m48\u001b[39m)\n",
      "File \u001b[0;32m~/py_venv/mmpose/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/py_venv/mmpose/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/pose_sandbox/Hand_pose_estimation_3D/arm_and_hand/ann.py:43\u001b[0m, in \u001b[0;36mANN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/py_venv/mmpose/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/py_venv/mmpose/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/py_venv/mmpose/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/py_venv/mmpose/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/py_venv/mmpose/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/py_venv/mmpose/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:156\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches_tracked \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_batches_tracked\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# use cumulative moving average\u001b[39;00m\n\u001b[1;32m    158\u001b[0m             exponential_average_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches_tracked)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "left_wrist_idx = 5\n",
    "num_left_arm_landmarks = 21\n",
    "#left_arm_idx = list(range(left_wrist_idx + 1, left_wrist_idx + num_left_arm_landmarks))\n",
    "left_arm_idx = [7, 8, 11, 12, 15, 16, 19, 20, 23, 24]\n",
    "\n",
    "train_losses, val_losses = train_model(model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    optimizer, \n",
    "    num_epochs=num_epochs, \n",
    "    save_path=save_path,\n",
    "    early_stopping=early_stopping,\n",
    "    scheduler=scheduler,\n",
    "    writer=writer,\n",
    "    log_seq=50,\n",
    "    train_left_arm_hand_only=True,\n",
    "    weight_idx=left_arm_idx,\n",
    "    weight=2.)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmpose",
   "language": "python",
   "name": "mmpose"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
