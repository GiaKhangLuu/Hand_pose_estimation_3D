{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/giakhang/dev/pose_sandbox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giakhang/anaconda3/envs/mmdeploy/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.abspath(os.curdir),\n",
    "                \"Hand_pose_estimation_3D/arm_and_hand\"))\n",
    "sys.path.append(os.path.join(os.path.abspath(os.curdir),\n",
    "                \"Hand_pose_estimation_3D\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from ann import ANN\n",
    "from dataloader_hand_only_ann import HandLandmarksDataset_ANN\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from csv_writer import columns_to_normalize, fusion_csv_columns_name\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "from landmarks_scaler import LandmarksScaler\n",
    "from train_ann_no_intrinsics import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hand_lmks = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = num_hand_lmks * 3 * 2 \n",
    "OUTPUT_DIM = num_hand_lmks * 3\n",
    "HIDDEN_DIM = num_hand_lmks * 3 \n",
    "NUM_HIDDEN_LAYERS = 3\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "model = ANN(input_dim=INPUT_DIM,\n",
    "            output_dim=OUTPUT_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            num_hidden_layers=NUM_HIDDEN_LAYERS,\n",
    "            dropout_rate=DROPOUT_RATE)\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"ann_left_hand\"\n",
    "DATETIME = \"{}\".format(datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "DATE = \"{}\".format(datetime.now().strftime(\"%Y%m%d\"))\n",
    "BASE_DIR = \"Hand_pose_estimation_3D/arm_and_hand/runs/{}\".format(MODEL_NAME)\n",
    "SAVE_DIR = os.path.join(BASE_DIR, DATE, DATETIME)\n",
    "DATA_DIR = \"data\"  \n",
    "writer = SummaryWriter(log_dir=SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_hand_fused_names = [\"left shoulder\", \"left elbow\", \"left hip\", \"right shoulder\",\n",
    "    \"right hip\", \"WRIST\", \"THUMB_CMC\", \"THUMB_MCP\", \"THUMB_IP\", \n",
    "    \"THUMB_TIP\", \"INDEX_FINGER_MCP\", \"INDEX_FINGER_PIP\", \"INDEX_FINGER_DIP\",\n",
    "    \"INDEX_FINGER_TIP\", \"MIDDLE_FINGER_MCP\", \"MIDDLE_FINGER_PIP\", \"MIDDLE_FINGER_DIP\",\n",
    "    \"MIDDLE_FINGER_TIP\", \"RING_FINGER_MCP\", \"RING_FINGER_PIP\", \"RING_FINGER_DIP\",\n",
    "    \"RING_FINGER_TIP\", \"PINKY_MCP\", \"PINKY_PIP\", \"PINKY_DIP\", \"PINKY_TIP\", \"right elbow\",\n",
    "    \"RIGHT_WRIST\", \"RIGHT_THUMB_CMC\", \"RIGHT_THUMB_MCP\", \"RIGHT_THUMB_IP\", \"RIGHT_THUMB_TIP\",\n",
    "    \"RIGHT_INDEX_FINGER_MCP\", \"RIGHT_INDEX_FINGER_PIP\", \"RIGHT_INDEX_FINGER_DIP\",\n",
    "    \"RIGHT_INDEX_FINGER_TIP\", \"RIGHT_MIDDLE_FINGER_MCP\", \"RIGHT_MIDDLE_FINGER_PIP\",\n",
    "    \"RIGHT_MIDDLE_FINGER_DIP\", \"RIGHT_MIDDLE_FINGER_TIP\", \"RIGHT_RING_FINGER_MCP\",\n",
    "    \"RIGHT_RING_FINGER_PIP\", \"RIGHT_RING_FINGER_DIP\", \"RIGHT_RING_FINGER_TIP\",\n",
    "    \"RIGHT_PINKY_MCP\", \"RIGHT_PINKY_PIP\", \"RIGHT_PINKY_DIP\", \"RIGHT_PINKY_TIP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"  \n",
    "SELECTED_DATE = \"2024-*\"  # Keep '*' when using glob.glob\n",
    "\n",
    "train_paths = glob.glob(os.path.join(DATA_DIR, \"{}/{}/fine_landmarks_{}_*.csv\".format(SELECTED_DATE, SELECTED_DATE, \"train\")))\n",
    "val_paths = glob.glob(os.path.join(DATA_DIR, \"{}/{}/fine_landmarks_{}_*.csv\".format(SELECTED_DATE, SELECTED_DATE, \"val\")))\n",
    "\n",
    "body_lines = [[0,2], [0, 3], [2, 4], [3, 4]]\n",
    "lefthand_lines = [[0, 1], [1, 5], [5, 6], [5, 10], [5, 22], [10, 14], [14, 18], [18, 22], \n",
    "    [6, 7], [7, 8], [8, 9], \n",
    "    [10, 11], [11, 12], [12, 13], \n",
    "    [14, 15], [15, 16], [16, 17], \n",
    "    [18, 19], [19, 20], [20, 21], \n",
    "    [22, 23], [23, 24], [24, 25]]\n",
    "train_body_distance_thres = 550\n",
    "train_leftarm_distance_thres = 550\n",
    "train_lefthand_distance_thres = 200\n",
    "val_body_distance_thres = 450\n",
    "val_leftarm_distance_thres = 450\n",
    "val_lefthand_distance_thres = 150\n",
    "\n",
    "input_scaler = MinMaxScaler()\n",
    "#output_scaler = MinMaxScaler()\n",
    "\n",
    "train_dataset = HandLandmarksDataset_ANN(train_paths, \n",
    "    arm_hand_fused_names,\n",
    "    body_lines, \n",
    "    lefthand_lines, \n",
    "    train_body_distance_thres, \n",
    "    train_leftarm_distance_thres, \n",
    "    train_lefthand_distance_thres,\n",
    "    filter_outlier=True,\n",
    "    only_keep_frames_contain_lefthand=True,\n",
    "    cvt_normalized_xy_to_XY=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hand_pose_estimation_3D/arm_and_hand/runs/ann_left_hand/20241029/20241029-1042/input_scaler.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_scaler.fit_transform(train_dataset._inputs)\n",
    "#output_scaler.fit_transform(train_dataset._outputs)\n",
    "\n",
    "input_scaler_save_path = os.path.join(SAVE_DIR, \"input_scaler.pkl\")\n",
    "#output_scaler_save_path = os.path.join(SAVE_DIR, \"output_scaler.pkl\")\n",
    "\n",
    "joblib.dump(input_scaler, input_scaler_save_path)\n",
    "#joblib.dump(output_scaler, output_scaler_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_scaler = LandmarksScaler(scaler_path=input_scaler_save_path)\n",
    "#output_scaler = LandmarksScaler(scaler_path=output_scaler_save_path)\n",
    "\n",
    "train_dataset = HandLandmarksDataset_ANN(train_paths, \n",
    "    arm_hand_fused_names,\n",
    "    body_lines, \n",
    "    lefthand_lines, \n",
    "    train_body_distance_thres, \n",
    "    train_leftarm_distance_thres, \n",
    "    train_lefthand_distance_thres,\n",
    "    filter_outlier=True,\n",
    "    only_keep_frames_contain_lefthand=True,\n",
    "    cvt_normalized_xy_to_XY=True,\n",
    "    input_scaler=input_scaler,\n",
    "    output_scaler=None)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = HandLandmarksDataset_ANN(val_paths,\n",
    "    arm_hand_fused_names,\n",
    "    body_lines,\n",
    "    lefthand_lines,\n",
    "    val_body_distance_thres,\n",
    "    val_leftarm_distance_thres,\n",
    "    val_lefthand_distance_thres,\n",
    "    filter_outlier=True,\n",
    "    only_keep_frames_contain_lefthand=True,\n",
    "    cvt_normalized_xy_to_XY=True,\n",
    "    input_scaler=input_scaler,\n",
    "    output_scaler=None)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 50000\n",
    "current_time = datetime.now().strftime('%Y%m%d-%H%M')\n",
    "save_path = os.path.join(SAVE_DIR, \"{}_{}_layers_best.pth\".format(MODEL_NAME, NUM_HIDDEN_LAYERS))\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', \n",
    "    factor=math.sqrt(0.1), patience=1000, verbose=True, min_lr=1e-8)\n",
    "early_stopping = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved with Validation Loss: 1141819.5970\n",
      "Model saved with Validation Loss: 1121240.2697\n",
      "Model saved with Validation Loss: 1102309.7697\n",
      "Model saved with Validation Loss: 1053962.1020\n",
      "Model saved with Validation Loss: 1052259.2549\n",
      "Model saved with Validation Loss: 1048055.1842\n",
      "Model saved with Validation Loss: 1037510.8273\n",
      "Model saved with Validation Loss: 1016048.0691\n",
      "Model saved with Validation Loss: 988761.6332\n",
      "Model saved with Validation Loss: 970512.4013\n",
      "Model saved with Validation Loss: 919305.3536\n",
      "Model saved with Validation Loss: 915603.1168\n",
      "Model saved with Validation Loss: 706226.7204\n",
      "Epoch 50/50000, Training Loss: 959896.2811\n",
      "Epoch 50/50000, Validation Loss: 986924.4112\n",
      "Model saved with Validation Loss: 695046.4852\n",
      "Epoch 100/50000, Training Loss: 805488.6357\n",
      "Epoch 100/50000, Validation Loss: 680492.3141\n",
      "Model saved with Validation Loss: 680492.3141\n",
      "Model saved with Validation Loss: 661540.4704\n",
      "Model saved with Validation Loss: 516646.9137\n",
      "Epoch 150/50000, Training Loss: 662567.5404\n",
      "Epoch 150/50000, Validation Loss: 617884.8339\n",
      "Model saved with Validation Loss: 481050.1390\n",
      "Model saved with Validation Loss: 478519.6472\n",
      "Model saved with Validation Loss: 417714.8750\n",
      "Epoch 200/50000, Training Loss: 536410.3671\n",
      "Epoch 200/50000, Validation Loss: 520539.5378\n",
      "Model saved with Validation Loss: 408881.2418\n",
      "Model saved with Validation Loss: 365740.5938\n",
      "Model saved with Validation Loss: 365237.3462\n",
      "Model saved with Validation Loss: 316230.4638\n",
      "Epoch 250/50000, Training Loss: 426020.2587\n",
      "Epoch 250/50000, Validation Loss: 359944.6620\n",
      "Model saved with Validation Loss: 265350.1414\n",
      "Model saved with Validation Loss: 260103.2928\n",
      "Model saved with Validation Loss: 260018.3421\n",
      "Epoch 300/50000, Training Loss: 330213.6731\n",
      "Epoch 300/50000, Validation Loss: 330055.5543\n",
      "Model saved with Validation Loss: 240416.9762\n",
      "Model saved with Validation Loss: 233638.6147\n",
      "Model saved with Validation Loss: 221469.1645\n",
      "Model saved with Validation Loss: 220823.1345\n",
      "Model saved with Validation Loss: 219990.4725\n",
      "Model saved with Validation Loss: 209527.3725\n",
      "Model saved with Validation Loss: 169891.8491\n",
      "Epoch 350/50000, Training Loss: 246703.9050\n",
      "Epoch 350/50000, Validation Loss: 251229.6192\n",
      "Model saved with Validation Loss: 159014.6110\n",
      "Model saved with Validation Loss: 158548.7611\n",
      "Model saved with Validation Loss: 158478.1484\n",
      "Model saved with Validation Loss: 130882.0284\n",
      "Model saved with Validation Loss: 130804.7743\n",
      "Model saved with Validation Loss: 123098.4344\n",
      "Epoch 400/50000, Training Loss: 177312.0033\n",
      "Epoch 400/50000, Validation Loss: 153483.2282\n",
      "Model saved with Validation Loss: 102979.0574\n",
      "Model saved with Validation Loss: 91122.0040\n",
      "Model saved with Validation Loss: 87950.4291\n",
      "Model saved with Validation Loss: 81346.0067\n",
      "Model saved with Validation Loss: 70054.1285\n",
      "Epoch 450/50000, Training Loss: 120772.9715\n",
      "Epoch 450/50000, Validation Loss: 109894.8699\n",
      "Model saved with Validation Loss: 69520.9587\n",
      "Model saved with Validation Loss: 65710.1181\n",
      "Model saved with Validation Loss: 55546.7829\n",
      "Model saved with Validation Loss: 49523.5333\n",
      "Epoch 500/50000, Training Loss: 78267.9948\n",
      "Epoch 500/50000, Validation Loss: 54112.7021\n",
      "Model saved with Validation Loss: 47897.1426\n",
      "Model saved with Validation Loss: 47660.2421\n",
      "Model saved with Validation Loss: 39916.0289\n",
      "Model saved with Validation Loss: 39095.4575\n",
      "Model saved with Validation Loss: 35942.9300\n",
      "Model saved with Validation Loss: 30797.2142\n",
      "Epoch 550/50000, Training Loss: 48604.4812\n",
      "Epoch 550/50000, Validation Loss: 35868.3785\n",
      "Model saved with Validation Loss: 28012.5825\n",
      "Model saved with Validation Loss: 25569.6436\n",
      "Model saved with Validation Loss: 22522.6006\n",
      "Model saved with Validation Loss: 20509.7808\n",
      "Model saved with Validation Loss: 19025.8757\n",
      "Model saved with Validation Loss: 18453.5505\n",
      "Epoch 600/50000, Training Loss: 31668.3749\n",
      "Epoch 600/50000, Validation Loss: 18009.3627\n",
      "Model saved with Validation Loss: 18009.3627\n",
      "Model saved with Validation Loss: 17548.1476\n",
      "Model saved with Validation Loss: 16234.5805\n",
      "Model saved with Validation Loss: 13320.7363\n",
      "Model saved with Validation Loss: 13252.6533\n",
      "Model saved with Validation Loss: 12278.6460\n",
      "Model saved with Validation Loss: 11792.1160\n",
      "Epoch 650/50000, Training Loss: 24385.5573\n",
      "Epoch 650/50000, Validation Loss: 23794.5715\n",
      "Model saved with Validation Loss: 11449.4808\n",
      "Model saved with Validation Loss: 10491.8779\n",
      "Model saved with Validation Loss: 10462.3482\n",
      "Model saved with Validation Loss: 9767.4203\n",
      "Epoch 700/50000, Training Loss: 22431.0119\n",
      "Epoch 700/50000, Validation Loss: 11045.1508\n",
      "Model saved with Validation Loss: 9324.6421\n",
      "Epoch 750/50000, Training Loss: 21346.8230\n",
      "Epoch 750/50000, Validation Loss: 21094.3229\n",
      "Model saved with Validation Loss: 9138.6526\n",
      "Epoch 800/50000, Training Loss: 20487.6181\n",
      "Epoch 800/50000, Validation Loss: 13311.9850\n",
      "Epoch 850/50000, Training Loss: 20075.2812\n",
      "Epoch 850/50000, Validation Loss: 11559.5095\n",
      "Model saved with Validation Loss: 9090.5861\n",
      "Epoch 900/50000, Training Loss: 19138.2841\n",
      "Epoch 900/50000, Validation Loss: 17952.0485\n",
      "Model saved with Validation Loss: 9049.2022\n",
      "Epoch 950/50000, Training Loss: 18545.8527\n",
      "Epoch 950/50000, Validation Loss: 20147.3076\n",
      "Model saved with Validation Loss: 8708.6735\n",
      "Model saved with Validation Loss: 8354.4084\n",
      "Model saved with Validation Loss: 8087.7357\n",
      "Epoch 1000/50000, Training Loss: 18122.9791\n",
      "Epoch 1000/50000, Validation Loss: 11827.8029\n",
      "Epoch 1050/50000, Training Loss: 18057.4770\n",
      "Epoch 1050/50000, Validation Loss: 10665.6113\n",
      "Epoch 1100/50000, Training Loss: 17857.5110\n",
      "Epoch 1100/50000, Validation Loss: 25801.1424\n",
      "Epoch 1150/50000, Training Loss: 17852.4553\n",
      "Epoch 1150/50000, Validation Loss: 9684.4960\n",
      "Epoch 1200/50000, Training Loss: 17645.7636\n",
      "Epoch 1200/50000, Validation Loss: 13276.9375\n",
      "Epoch 1250/50000, Training Loss: 17496.8379\n",
      "Epoch 1250/50000, Validation Loss: 13555.7861\n",
      "Model saved with Validation Loss: 7940.6476\n",
      "Epoch 1300/50000, Training Loss: 17072.8157\n",
      "Epoch 1300/50000, Validation Loss: 10837.7342\n",
      "Epoch 1350/50000, Training Loss: 17357.7438\n",
      "Epoch 1350/50000, Validation Loss: 11198.0375\n",
      "Epoch 1400/50000, Training Loss: 17000.6479\n",
      "Epoch 1400/50000, Validation Loss: 8604.5508\n",
      "Model saved with Validation Loss: 7839.6479\n",
      "Model saved with Validation Loss: 7723.6124\n",
      "Epoch 1450/50000, Training Loss: 16898.4655\n",
      "Epoch 1450/50000, Validation Loss: 19879.1270\n",
      "Epoch 1500/50000, Training Loss: 16570.1611\n",
      "Epoch 1500/50000, Validation Loss: 10820.6183\n",
      "Model saved with Validation Loss: 7722.2352\n",
      "Epoch 1550/50000, Training Loss: 16571.3597\n",
      "Epoch 1550/50000, Validation Loss: 15934.9573\n",
      "Epoch 1600/50000, Training Loss: 16254.7724\n",
      "Epoch 1600/50000, Validation Loss: 19099.7809\n",
      "Epoch 1650/50000, Training Loss: 16235.6634\n",
      "Epoch 1650/50000, Validation Loss: 13894.8661\n",
      "Model saved with Validation Loss: 6735.3868\n",
      "Epoch 1700/50000, Training Loss: 16275.9965\n",
      "Epoch 1700/50000, Validation Loss: 9870.3164\n",
      "Epoch 1750/50000, Training Loss: 16213.8393\n",
      "Epoch 1750/50000, Validation Loss: 8831.2989\n",
      "Epoch 1800/50000, Training Loss: 16090.2765\n",
      "Epoch 1800/50000, Validation Loss: 9022.6733\n",
      "Epoch 1850/50000, Training Loss: 15994.1207\n",
      "Epoch 1850/50000, Validation Loss: 15008.8084\n",
      "Epoch 1900/50000, Training Loss: 15722.9359\n",
      "Epoch 1900/50000, Validation Loss: 9760.9259\n",
      "Epoch 1950/50000, Training Loss: 16017.7336\n",
      "Epoch 1950/50000, Validation Loss: 9304.2069\n",
      "Epoch 2000/50000, Training Loss: 15711.4986\n",
      "Epoch 2000/50000, Validation Loss: 12692.2287\n",
      "Epoch 2050/50000, Training Loss: 15713.0526\n",
      "Epoch 2050/50000, Validation Loss: 9025.1069\n",
      "Epoch 2100/50000, Training Loss: 15590.0213\n",
      "Epoch 2100/50000, Validation Loss: 17098.4765\n",
      "Epoch 2150/50000, Training Loss: 15524.2879\n",
      "Epoch 2150/50000, Validation Loss: 8958.0442\n",
      "Epoch 2200/50000, Training Loss: 15349.0325\n",
      "Epoch 2200/50000, Validation Loss: 8770.1350\n",
      "Epoch 2250/50000, Training Loss: 15648.8386\n",
      "Epoch 2250/50000, Validation Loss: 19654.0904\n",
      "Epoch 2300/50000, Training Loss: 15359.7857\n",
      "Epoch 2300/50000, Validation Loss: 10994.5817\n",
      "Epoch 2350/50000, Training Loss: 15285.1007\n",
      "Epoch 2350/50000, Validation Loss: 7908.9984\n",
      "Epoch 2400/50000, Training Loss: 15343.5284\n",
      "Epoch 2400/50000, Validation Loss: 9460.3752\n",
      "Epoch 2450/50000, Training Loss: 14913.8117\n",
      "Epoch 2450/50000, Validation Loss: 8844.8465\n",
      "Epoch 2500/50000, Training Loss: 15213.1233\n",
      "Epoch 2500/50000, Validation Loss: 16287.6906\n",
      "Epoch 2550/50000, Training Loss: 15133.7061\n",
      "Epoch 2550/50000, Validation Loss: 12068.6048\n",
      "Epoch 2600/50000, Training Loss: 14911.2300\n",
      "Epoch 2600/50000, Validation Loss: 7865.1352\n",
      "Epoch 2650/50000, Training Loss: 15056.4708\n",
      "Epoch 2650/50000, Validation Loss: 8757.9639\n",
      "Epoch 02675: reducing learning rate of group 0 to 3.1623e-04.\n",
      "Epoch 2700/50000, Training Loss: 14964.8099\n",
      "Epoch 2700/50000, Validation Loss: 10093.2545\n",
      "Epoch 2750/50000, Training Loss: 14805.8210\n",
      "Epoch 2750/50000, Validation Loss: 9083.3779\n",
      "Epoch 2800/50000, Training Loss: 14943.0716\n",
      "Epoch 2800/50000, Validation Loss: 8479.5742\n",
      "Epoch 2850/50000, Training Loss: 14639.8418\n",
      "Epoch 2850/50000, Validation Loss: 10512.3482\n",
      "Epoch 2900/50000, Training Loss: 14756.1413\n",
      "Epoch 2900/50000, Validation Loss: 8445.2171\n",
      "Epoch 2950/50000, Training Loss: 14685.8004\n",
      "Epoch 2950/50000, Validation Loss: 9624.4585\n",
      "Epoch 3000/50000, Training Loss: 14515.1521\n",
      "Epoch 3000/50000, Validation Loss: 9361.2558\n",
      "Epoch 3050/50000, Training Loss: 14657.3180\n",
      "Epoch 3050/50000, Validation Loss: 8451.4088\n",
      "Epoch 3100/50000, Training Loss: 14487.9381\n",
      "Epoch 3100/50000, Validation Loss: 9980.6977\n",
      "Epoch 3150/50000, Training Loss: 14486.3692\n",
      "Epoch 3150/50000, Validation Loss: 8982.3591\n",
      "Epoch 3200/50000, Training Loss: 14578.5239\n",
      "Epoch 3200/50000, Validation Loss: 9337.2966\n",
      "Epoch 3250/50000, Training Loss: 14451.2471\n",
      "Epoch 3250/50000, Validation Loss: 8828.5594\n",
      "Epoch 3300/50000, Training Loss: 14515.5237\n",
      "Epoch 3300/50000, Validation Loss: 7962.0901\n",
      "Epoch 3350/50000, Training Loss: 14257.3634\n",
      "Epoch 3350/50000, Validation Loss: 10750.0988\n",
      "Epoch 3400/50000, Training Loss: 14499.2022\n",
      "Epoch 3400/50000, Validation Loss: 8697.6235\n",
      "Epoch 3450/50000, Training Loss: 14675.5568\n",
      "Epoch 3450/50000, Validation Loss: 8596.8887\n",
      "Epoch 3500/50000, Training Loss: 14545.9153\n",
      "Epoch 3500/50000, Validation Loss: 9056.6636\n",
      "Epoch 3550/50000, Training Loss: 14505.3255\n",
      "Epoch 3550/50000, Validation Loss: 8284.8168\n",
      "Epoch 3600/50000, Training Loss: 14579.6338\n",
      "Epoch 3600/50000, Validation Loss: 7980.7649\n",
      "Epoch 3650/50000, Training Loss: 14392.0127\n",
      "Epoch 3650/50000, Validation Loss: 8349.5189\n",
      "Epoch 03676: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 3700/50000, Training Loss: 14485.2933\n",
      "Epoch 3700/50000, Validation Loss: 8846.9614\n",
      "Epoch 3750/50000, Training Loss: 14320.7992\n",
      "Epoch 3750/50000, Validation Loss: 9123.2167\n",
      "Epoch 3800/50000, Training Loss: 14192.2035\n",
      "Epoch 3800/50000, Validation Loss: 9064.9081\n",
      "Epoch 3850/50000, Training Loss: 14192.8560\n",
      "Epoch 3850/50000, Validation Loss: 8768.5391\n",
      "Epoch 3900/50000, Training Loss: 14466.5478\n",
      "Epoch 3900/50000, Validation Loss: 9216.7938\n",
      "Epoch 3950/50000, Training Loss: 14050.4562\n",
      "Epoch 3950/50000, Validation Loss: 9517.4184\n",
      "Epoch 4000/50000, Training Loss: 14391.6137\n",
      "Epoch 4000/50000, Validation Loss: 11037.8426\n",
      "Epoch 4050/50000, Training Loss: 14394.9555\n",
      "Epoch 4050/50000, Validation Loss: 8869.8651\n",
      "Epoch 4100/50000, Training Loss: 14538.6531\n",
      "Epoch 4100/50000, Validation Loss: 9099.0816\n",
      "Epoch 4150/50000, Training Loss: 14230.3657\n",
      "Epoch 4150/50000, Validation Loss: 9409.3306\n",
      "Epoch 4200/50000, Training Loss: 14326.4464\n",
      "Epoch 4200/50000, Validation Loss: 8937.1323\n",
      "Epoch 4250/50000, Training Loss: 14355.7450\n",
      "Epoch 4250/50000, Validation Loss: 9039.6780\n",
      "Epoch 4300/50000, Training Loss: 14366.8412\n",
      "Epoch 4300/50000, Validation Loss: 8900.6931\n",
      "Epoch 4350/50000, Training Loss: 13953.9580\n",
      "Epoch 4350/50000, Validation Loss: 9398.5973\n",
      "Epoch 4400/50000, Training Loss: 14332.4539\n",
      "Epoch 4400/50000, Validation Loss: 10318.1846\n",
      "Epoch 4450/50000, Training Loss: 14244.8953\n",
      "Epoch 4450/50000, Validation Loss: 9121.1153\n",
      "Epoch 4500/50000, Training Loss: 14214.1825\n",
      "Epoch 4500/50000, Validation Loss: 9667.4417\n",
      "Epoch 4550/50000, Training Loss: 14165.4957\n",
      "Epoch 4550/50000, Validation Loss: 9186.5663\n",
      "Epoch 4600/50000, Training Loss: 14267.0792\n",
      "Epoch 4600/50000, Validation Loss: 8818.2797\n",
      "Epoch 4650/50000, Training Loss: 14138.8615\n",
      "Epoch 4650/50000, Validation Loss: 8398.4966\n",
      "Epoch 04677: reducing learning rate of group 0 to 3.1623e-05.\n",
      "Epoch 4700/50000, Training Loss: 14138.6836\n",
      "Epoch 4700/50000, Validation Loss: 11048.8893\n",
      "Epoch 4750/50000, Training Loss: 14221.6835\n",
      "Epoch 4750/50000, Validation Loss: 8712.1868\n",
      "Epoch 4800/50000, Training Loss: 14047.6091\n",
      "Epoch 4800/50000, Validation Loss: 9276.7340\n",
      "Epoch 4850/50000, Training Loss: 14385.6795\n",
      "Epoch 4850/50000, Validation Loss: 9318.9896\n",
      "Epoch 4900/50000, Training Loss: 14105.6855\n",
      "Epoch 4900/50000, Validation Loss: 9010.7134\n",
      "Epoch 4950/50000, Training Loss: 14076.0301\n",
      "Epoch 4950/50000, Validation Loss: 8666.8252\n",
      "Epoch 5000/50000, Training Loss: 14206.8937\n",
      "Epoch 5000/50000, Validation Loss: 9118.8555\n",
      "Epoch 5050/50000, Training Loss: 14100.1452\n",
      "Epoch 5050/50000, Validation Loss: 9180.6781\n",
      "Epoch 5100/50000, Training Loss: 14159.3692\n",
      "Epoch 5100/50000, Validation Loss: 8766.4355\n",
      "Epoch 5150/50000, Training Loss: 14211.1504\n",
      "Epoch 5150/50000, Validation Loss: 8850.2857\n",
      "Epoch 5200/50000, Training Loss: 14269.3852\n",
      "Epoch 5200/50000, Validation Loss: 9472.1302\n",
      "Epoch 5250/50000, Training Loss: 14105.4817\n",
      "Epoch 5250/50000, Validation Loss: 9374.0519\n",
      "Epoch 5300/50000, Training Loss: 14172.9508\n",
      "Epoch 5300/50000, Validation Loss: 8833.1409\n",
      "Epoch 5350/50000, Training Loss: 14105.6484\n",
      "Epoch 5350/50000, Validation Loss: 8607.5172\n",
      "Epoch 5400/50000, Training Loss: 14221.2720\n",
      "Epoch 5400/50000, Validation Loss: 8674.5201\n",
      "Epoch 5450/50000, Training Loss: 14073.3911\n",
      "Epoch 5450/50000, Validation Loss: 9157.2138\n",
      "Epoch 5500/50000, Training Loss: 14165.7930\n",
      "Epoch 5500/50000, Validation Loss: 9579.6709\n",
      "Epoch 5550/50000, Training Loss: 14093.2242\n",
      "Epoch 5550/50000, Validation Loss: 8817.7450\n",
      "Epoch 5600/50000, Training Loss: 14145.8704\n",
      "Epoch 5600/50000, Validation Loss: 8419.0290\n",
      "Epoch 5650/50000, Training Loss: 14299.9083\n",
      "Epoch 5650/50000, Validation Loss: 9086.2023\n",
      "Epoch 05678: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 5700/50000, Training Loss: 14163.1062\n",
      "Epoch 5700/50000, Validation Loss: 8628.0313\n",
      "Epoch 5750/50000, Training Loss: 14347.5345\n",
      "Epoch 5750/50000, Validation Loss: 9072.8880\n",
      "Epoch 5800/50000, Training Loss: 14028.8565\n",
      "Epoch 5800/50000, Validation Loss: 9614.3696\n",
      "Epoch 5850/50000, Training Loss: 14346.2420\n",
      "Epoch 5850/50000, Validation Loss: 9116.4641\n",
      "Epoch 5900/50000, Training Loss: 14153.1998\n",
      "Epoch 5900/50000, Validation Loss: 9639.0245\n",
      "Epoch 5950/50000, Training Loss: 14324.8340\n",
      "Epoch 5950/50000, Validation Loss: 8738.8037\n",
      "Epoch 6000/50000, Training Loss: 14370.3162\n",
      "Epoch 6000/50000, Validation Loss: 10110.0748\n",
      "Epoch 6050/50000, Training Loss: 14088.5931\n",
      "Epoch 6050/50000, Validation Loss: 9018.1950\n",
      "Epoch 6100/50000, Training Loss: 14248.5831\n",
      "Epoch 6100/50000, Validation Loss: 8855.5171\n",
      "Epoch 6150/50000, Training Loss: 14053.7243\n",
      "Epoch 6150/50000, Validation Loss: 9056.9083\n",
      "Epoch 6200/50000, Training Loss: 14243.5579\n",
      "Epoch 6200/50000, Validation Loss: 8870.5811\n",
      "Epoch 6250/50000, Training Loss: 13944.7845\n",
      "Epoch 6250/50000, Validation Loss: 9100.8660\n",
      "Epoch 6300/50000, Training Loss: 13999.7662\n",
      "Epoch 6300/50000, Validation Loss: 8807.0242\n",
      "Epoch 6350/50000, Training Loss: 13928.6297\n",
      "Epoch 6350/50000, Validation Loss: 9073.8554\n",
      "Epoch 6400/50000, Training Loss: 14113.2578\n",
      "Epoch 6400/50000, Validation Loss: 9365.1158\n",
      "Epoch 6450/50000, Training Loss: 14254.7405\n",
      "Epoch 6450/50000, Validation Loss: 8641.4145\n",
      "Epoch 6500/50000, Training Loss: 14141.1237\n",
      "Epoch 6500/50000, Validation Loss: 8502.4874\n",
      "Epoch 6550/50000, Training Loss: 14187.5332\n",
      "Epoch 6550/50000, Validation Loss: 8778.5145\n",
      "Epoch 6600/50000, Training Loss: 14089.8780\n",
      "Epoch 6600/50000, Validation Loss: 8599.3738\n",
      "Epoch 6650/50000, Training Loss: 14367.3750\n",
      "Epoch 6650/50000, Validation Loss: 9045.2155\n",
      "Epoch 06679: reducing learning rate of group 0 to 3.1623e-06.\n",
      "Epoch 6700/50000, Training Loss: 13983.4895\n",
      "Epoch 6700/50000, Validation Loss: 8848.5913\n",
      "Epoch 6750/50000, Training Loss: 14158.7949\n",
      "Epoch 6750/50000, Validation Loss: 8831.0397\n",
      "Epoch 6800/50000, Training Loss: 14252.3604\n",
      "Epoch 6800/50000, Validation Loss: 9360.1099\n",
      "Epoch 6850/50000, Training Loss: 13967.0997\n",
      "Epoch 6850/50000, Validation Loss: 9200.1012\n",
      "Epoch 6900/50000, Training Loss: 14133.5077\n",
      "Epoch 6900/50000, Validation Loss: 8929.6821\n",
      "Epoch 6950/50000, Training Loss: 14204.7705\n",
      "Epoch 6950/50000, Validation Loss: 8797.4133\n",
      "Epoch 7000/50000, Training Loss: 14088.8305\n",
      "Epoch 7000/50000, Validation Loss: 8947.8393\n",
      "Epoch 7050/50000, Training Loss: 14102.9620\n",
      "Epoch 7050/50000, Validation Loss: 9616.6908\n",
      "Epoch 7100/50000, Training Loss: 14353.9851\n",
      "Epoch 7100/50000, Validation Loss: 8814.4918\n",
      "Epoch 7150/50000, Training Loss: 14071.7842\n",
      "Epoch 7150/50000, Validation Loss: 9226.2082\n",
      "Epoch 7200/50000, Training Loss: 14226.2399\n",
      "Epoch 7200/50000, Validation Loss: 9276.9603\n",
      "Epoch 7250/50000, Training Loss: 14187.5177\n",
      "Epoch 7250/50000, Validation Loss: 9074.7722\n",
      "Epoch 7300/50000, Training Loss: 14042.8881\n",
      "Epoch 7300/50000, Validation Loss: 8990.9223\n",
      "Epoch 7350/50000, Training Loss: 14240.9488\n",
      "Epoch 7350/50000, Validation Loss: 8922.8859\n",
      "Epoch 7400/50000, Training Loss: 14242.9413\n",
      "Epoch 7400/50000, Validation Loss: 9123.2794\n",
      "Epoch 7450/50000, Training Loss: 14278.1089\n",
      "Epoch 7450/50000, Validation Loss: 9090.5210\n",
      "Epoch 7500/50000, Training Loss: 14131.5483\n",
      "Epoch 7500/50000, Validation Loss: 9042.5212\n",
      "Epoch 7550/50000, Training Loss: 14122.0561\n",
      "Epoch 7550/50000, Validation Loss: 9123.7637\n",
      "Epoch 7600/50000, Training Loss: 14158.6521\n",
      "Epoch 7600/50000, Validation Loss: 9096.0843\n",
      "Epoch 7650/50000, Training Loss: 14206.7515\n",
      "Epoch 7650/50000, Validation Loss: 8987.0501\n",
      "Epoch 07680: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 7700/50000, Training Loss: 13969.2098\n",
      "Epoch 7700/50000, Validation Loss: 8967.3606\n",
      "Epoch 7750/50000, Training Loss: 14111.0115\n",
      "Epoch 7750/50000, Validation Loss: 9188.5959\n",
      "Epoch 7800/50000, Training Loss: 14222.7335\n",
      "Epoch 7800/50000, Validation Loss: 9250.4709\n",
      "Epoch 7850/50000, Training Loss: 14197.2495\n",
      "Epoch 7850/50000, Validation Loss: 8858.8414\n",
      "Epoch 7900/50000, Training Loss: 14098.1404\n",
      "Epoch 7900/50000, Validation Loss: 8773.5363\n",
      "Epoch 7950/50000, Training Loss: 14301.2562\n",
      "Epoch 7950/50000, Validation Loss: 9639.7242\n",
      "Epoch 8000/50000, Training Loss: 14334.2208\n",
      "Epoch 8000/50000, Validation Loss: 8539.1691\n",
      "Epoch 8050/50000, Training Loss: 14278.2500\n",
      "Epoch 8050/50000, Validation Loss: 8781.7568\n",
      "Epoch 8100/50000, Training Loss: 14004.6681\n",
      "Epoch 8100/50000, Validation Loss: 8880.1270\n",
      "Epoch 8150/50000, Training Loss: 14273.1354\n",
      "Epoch 8150/50000, Validation Loss: 8863.4461\n",
      "Epoch 8200/50000, Training Loss: 14156.4779\n",
      "Epoch 8200/50000, Validation Loss: 8859.1978\n",
      "Epoch 8250/50000, Training Loss: 13897.9871\n",
      "Epoch 8250/50000, Validation Loss: 9013.1195\n",
      "Epoch 8300/50000, Training Loss: 14326.2609\n",
      "Epoch 8300/50000, Validation Loss: 9097.1228\n",
      "Epoch 8350/50000, Training Loss: 14032.3351\n",
      "Epoch 8350/50000, Validation Loss: 9253.7097\n",
      "Epoch 8400/50000, Training Loss: 14221.2162\n",
      "Epoch 8400/50000, Validation Loss: 8742.5871\n",
      "Epoch 8450/50000, Training Loss: 14100.6154\n",
      "Epoch 8450/50000, Validation Loss: 9115.9754\n",
      "Epoch 8500/50000, Training Loss: 14092.0817\n",
      "Epoch 8500/50000, Validation Loss: 8531.4524\n",
      "Epoch 8550/50000, Training Loss: 14304.8960\n",
      "Epoch 8550/50000, Validation Loss: 9044.8369\n",
      "Epoch 8600/50000, Training Loss: 14153.4471\n",
      "Epoch 8600/50000, Validation Loss: 9058.1857\n",
      "Epoch 8650/50000, Training Loss: 14121.5845\n",
      "Epoch 8650/50000, Validation Loss: 9190.8335\n",
      "Epoch 08681: reducing learning rate of group 0 to 3.1623e-07.\n",
      "Epoch 8700/50000, Training Loss: 14090.5418\n",
      "Epoch 8700/50000, Validation Loss: 9347.5314\n",
      "Epoch 8750/50000, Training Loss: 14186.8485\n",
      "Epoch 8750/50000, Validation Loss: 9001.1721\n",
      "Epoch 8800/50000, Training Loss: 14118.0832\n",
      "Epoch 8800/50000, Validation Loss: 8798.8057\n",
      "Epoch 8850/50000, Training Loss: 14168.7378\n",
      "Epoch 8850/50000, Validation Loss: 8981.7590\n",
      "Epoch 8900/50000, Training Loss: 14132.8531\n",
      "Epoch 8900/50000, Validation Loss: 8753.7727\n",
      "Epoch 8950/50000, Training Loss: 14211.3043\n",
      "Epoch 8950/50000, Validation Loss: 8850.2450\n",
      "Epoch 9000/50000, Training Loss: 14320.7330\n",
      "Epoch 9000/50000, Validation Loss: 8917.6365\n",
      "Epoch 9050/50000, Training Loss: 14065.7541\n",
      "Epoch 9050/50000, Validation Loss: 8741.7173\n",
      "Epoch 9100/50000, Training Loss: 14126.2484\n",
      "Epoch 9100/50000, Validation Loss: 9200.9723\n",
      "Epoch 9150/50000, Training Loss: 14178.7339\n",
      "Epoch 9150/50000, Validation Loss: 9311.8270\n",
      "Epoch 9200/50000, Training Loss: 14036.0337\n",
      "Epoch 9200/50000, Validation Loss: 9007.8534\n",
      "Epoch 9250/50000, Training Loss: 14125.1305\n",
      "Epoch 9250/50000, Validation Loss: 8979.7901\n",
      "Epoch 9300/50000, Training Loss: 14120.3607\n",
      "Epoch 9300/50000, Validation Loss: 8762.8832\n",
      "Epoch 9350/50000, Training Loss: 14094.0919\n",
      "Epoch 9350/50000, Validation Loss: 8911.2268\n",
      "Epoch 9400/50000, Training Loss: 14153.2904\n",
      "Epoch 9400/50000, Validation Loss: 9161.9736\n",
      "Epoch 9450/50000, Training Loss: 14211.7415\n",
      "Epoch 9450/50000, Validation Loss: 8886.1913\n",
      "Epoch 9500/50000, Training Loss: 14098.3329\n",
      "Epoch 9500/50000, Validation Loss: 8948.8322\n",
      "Epoch 9550/50000, Training Loss: 14127.1829\n",
      "Epoch 9550/50000, Validation Loss: 8555.3763\n",
      "Epoch 9600/50000, Training Loss: 14376.4272\n",
      "Epoch 9600/50000, Validation Loss: 9102.7831\n",
      "Epoch 9650/50000, Training Loss: 14071.8817\n",
      "Epoch 9650/50000, Validation Loss: 8795.5104\n",
      "Epoch 09682: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 9700/50000, Training Loss: 14006.2868\n",
      "Epoch 9700/50000, Validation Loss: 8782.1969\n",
      "Epoch 9750/50000, Training Loss: 14211.7760\n",
      "Epoch 9750/50000, Validation Loss: 8894.3066\n",
      "Epoch 9800/50000, Training Loss: 14172.9125\n",
      "Epoch 9800/50000, Validation Loss: 9398.1777\n",
      "Epoch 9850/50000, Training Loss: 14257.0338\n",
      "Epoch 9850/50000, Validation Loss: 9353.8308\n",
      "Epoch 9900/50000, Training Loss: 14249.6859\n",
      "Epoch 9900/50000, Validation Loss: 8559.3806\n",
      "Epoch 9950/50000, Training Loss: 14191.5092\n",
      "Epoch 9950/50000, Validation Loss: 8822.4885\n",
      "Epoch 10000/50000, Training Loss: 14203.7334\n",
      "Epoch 10000/50000, Validation Loss: 8517.4499\n",
      "Epoch 10050/50000, Training Loss: 14170.4218\n",
      "Epoch 10050/50000, Validation Loss: 9012.0116\n",
      "Epoch 10100/50000, Training Loss: 14211.9647\n",
      "Epoch 10100/50000, Validation Loss: 8970.3915\n",
      "Epoch 10150/50000, Training Loss: 14260.0302\n",
      "Epoch 10150/50000, Validation Loss: 8743.0235\n",
      "Epoch 10200/50000, Training Loss: 14206.1122\n",
      "Epoch 10200/50000, Validation Loss: 9154.7933\n",
      "Epoch 10250/50000, Training Loss: 14077.4993\n",
      "Epoch 10250/50000, Validation Loss: 8842.2637\n",
      "Epoch 10300/50000, Training Loss: 14257.1184\n",
      "Epoch 10300/50000, Validation Loss: 9272.1735\n",
      "Epoch 10350/50000, Training Loss: 14249.1216\n",
      "Epoch 10350/50000, Validation Loss: 8506.9944\n",
      "Epoch 10400/50000, Training Loss: 14426.7337\n",
      "Epoch 10400/50000, Validation Loss: 8861.1111\n",
      "Epoch 10450/50000, Training Loss: 14175.5422\n",
      "Epoch 10450/50000, Validation Loss: 8833.2010\n",
      "Epoch 10500/50000, Training Loss: 14202.0550\n",
      "Epoch 10500/50000, Validation Loss: 9333.8545\n",
      "Epoch 10550/50000, Training Loss: 14176.9879\n",
      "Epoch 10550/50000, Validation Loss: 8711.0200\n",
      "Epoch 10600/50000, Training Loss: 14066.6888\n",
      "Epoch 10600/50000, Validation Loss: 8899.5306\n",
      "Epoch 10650/50000, Training Loss: 13945.1499\n",
      "Epoch 10650/50000, Validation Loss: 9315.1833\n",
      "Epoch 10683: reducing learning rate of group 0 to 3.1623e-08.\n",
      "Epoch 10700/50000, Training Loss: 14256.4657\n",
      "Epoch 10700/50000, Validation Loss: 8969.2334\n",
      "Epoch 10750/50000, Training Loss: 14290.0313\n",
      "Epoch 10750/50000, Validation Loss: 8725.6459\n",
      "Epoch 10800/50000, Training Loss: 14184.2033\n",
      "Epoch 10800/50000, Validation Loss: 8675.0810\n",
      "Epoch 10850/50000, Training Loss: 14094.6497\n",
      "Epoch 10850/50000, Validation Loss: 9045.0795\n",
      "Epoch 10900/50000, Training Loss: 14072.1769\n",
      "Epoch 10900/50000, Validation Loss: 8772.8191\n",
      "Epoch 10950/50000, Training Loss: 14114.6773\n",
      "Epoch 10950/50000, Validation Loss: 8983.1992\n",
      "Epoch 11000/50000, Training Loss: 14144.6124\n",
      "Epoch 11000/50000, Validation Loss: 9286.7722\n",
      "Epoch 11050/50000, Training Loss: 14118.3293\n",
      "Epoch 11050/50000, Validation Loss: 8843.0981\n",
      "Epoch 11100/50000, Training Loss: 14291.8575\n",
      "Epoch 11100/50000, Validation Loss: 8582.9432\n",
      "Epoch 11150/50000, Training Loss: 14301.3913\n",
      "Epoch 11150/50000, Validation Loss: 8971.7413\n",
      "Epoch 11200/50000, Training Loss: 13932.6914\n",
      "Epoch 11200/50000, Validation Loss: 8990.5963\n",
      "Epoch 11250/50000, Training Loss: 14301.3097\n",
      "Epoch 11250/50000, Validation Loss: 9066.5352\n",
      "Epoch 11300/50000, Training Loss: 14137.6628\n",
      "Epoch 11300/50000, Validation Loss: 9215.9658\n",
      "Epoch 11350/50000, Training Loss: 14126.5125\n",
      "Epoch 11350/50000, Validation Loss: 8850.0707\n",
      "Epoch 11400/50000, Training Loss: 13980.0598\n",
      "Epoch 11400/50000, Validation Loss: 9162.8401\n",
      "Epoch 11450/50000, Training Loss: 14051.4171\n",
      "Epoch 11450/50000, Validation Loss: 8959.6500\n",
      "Epoch 11500/50000, Training Loss: 14270.7313\n",
      "Epoch 11500/50000, Validation Loss: 9201.5650\n",
      "Epoch 11550/50000, Training Loss: 14203.2378\n",
      "Epoch 11550/50000, Validation Loss: 8526.1835\n",
      "Epoch 11600/50000, Training Loss: 14168.4957\n",
      "Epoch 11600/50000, Validation Loss: 8626.0261\n",
      "Epoch 11650/50000, Training Loss: 13946.6129\n",
      "Epoch 11650/50000, Validation Loss: 8492.5583\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m train_model(model, \n\u001b[1;32m      2\u001b[0m     train_dataloader, \n\u001b[1;32m      3\u001b[0m     val_dataloader, \n\u001b[1;32m      4\u001b[0m     optimizer, \n\u001b[1;32m      5\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs, \n\u001b[1;32m      6\u001b[0m     save_path\u001b[38;5;241m=\u001b[39msave_path,\n\u001b[1;32m      7\u001b[0m     early_stopping\u001b[38;5;241m=\u001b[39mearly_stopping,\n\u001b[1;32m      8\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[1;32m      9\u001b[0m     writer\u001b[38;5;241m=\u001b[39mwriter,\n\u001b[1;32m     10\u001b[0m     log_seq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     12\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/dev/pose_sandbox/Hand_pose_estimation_3D/arm_and_hand/train_ann_no_intrinsics.py:136\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, num_epochs, save_path, early_stopping, scheduler, writer, log_seq)\u001b[0m\n\u001b[1;32m    133\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    134\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 136\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\u001b[38;5;241m.\u001b[39mitem() \n\u001b[1;32m    138\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[1;32m    139\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train_model(model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    optimizer, \n",
    "    num_epochs=num_epochs, \n",
    "    save_path=save_path,\n",
    "    early_stopping=early_stopping,\n",
    "    scheduler=scheduler,\n",
    "    writer=writer,\n",
    "    log_seq=50)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmdeploy",
   "language": "python",
   "name": "mmdeploy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
