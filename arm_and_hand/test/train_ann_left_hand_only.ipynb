{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/giakhang/dev/pose_sandbox\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.abspath(os.curdir),\n",
    "                \"Hand_pose_estimation_3D/arm_and_hand\"))\n",
    "sys.path.append(os.path.join(os.path.abspath(os.curdir),\n",
    "                \"Hand_pose_estimation_3D\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from transformer_encoder import TransformerEncoder\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from ann import ANN\n",
    "from dataloader_hand_only_ann import HandLandmarksDataset_ANN\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from csv_writer import columns_to_normalize, fusion_csv_columns_name\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "from landmarks_scaler import LandmarksScaler\n",
    "from train_ann_no_intrinsics import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hand_lmks = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = num_hand_lmks * 3 * 2 \n",
    "OUTPUT_DIM = num_hand_lmks * 3\n",
    "HIDDEN_DIM = num_hand_lmks * 3 \n",
    "NUM_HIDDEN_LAYERS = 3\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "model = ANN(input_dim=INPUT_DIM,\n",
    "            output_dim=OUTPUT_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            num_hidden_layers=NUM_HIDDEN_LAYERS,\n",
    "            dropout_rate=DROPOUT_RATE)\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"ann_left_hand\"\n",
    "DATETIME = \"{}\".format(datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "DATE = \"{}\".format(datetime.now().strftime(\"%Y%m%d\"))\n",
    "BASE_DIR = \"/home/giakhang/dev/pose_sandbox/Hand_pose_estimation_3D/arm_and_hand/runs/{}\".format(MODEL_NAME)\n",
    "SAVE_DIR = os.path.join(BASE_DIR, DATE, DATETIME)\n",
    "DATA_DIR = \"/home/giakhang/dev/pose_sandbox/data\"  \n",
    "writer = SummaryWriter(log_dir=SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_hand_fused_names = [\"left shoulder\", \"left elbow\", \"left hip\", \"right shoulder\",\n",
    "    \"right hip\", \"WRIST\", \"THUMB_CMC\", \"THUMB_MCP\", \"THUMB_IP\", \n",
    "    \"THUMB_TIP\", \"INDEX_FINGER_MCP\", \"INDEX_FINGER_PIP\", \"INDEX_FINGER_DIP\",\n",
    "    \"INDEX_FINGER_TIP\", \"MIDDLE_FINGER_MCP\", \"MIDDLE_FINGER_PIP\", \"MIDDLE_FINGER_DIP\",\n",
    "    \"MIDDLE_FINGER_TIP\", \"RING_FINGER_MCP\", \"RING_FINGER_PIP\", \"RING_FINGER_DIP\",\n",
    "    \"RING_FINGER_TIP\", \"PINKY_MCP\", \"PINKY_PIP\", \"PINKY_DIP\", \"PINKY_TIP\", \"right elbow\",\n",
    "    \"RIGHT_WRIST\", \"RIGHT_THUMB_CMC\", \"RIGHT_THUMB_MCP\", \"RIGHT_THUMB_IP\", \"RIGHT_THUMB_TIP\",\n",
    "    \"RIGHT_INDEX_FINGER_MCP\", \"RIGHT_INDEX_FINGER_PIP\", \"RIGHT_INDEX_FINGER_DIP\",\n",
    "    \"RIGHT_INDEX_FINGER_TIP\", \"RIGHT_MIDDLE_FINGER_MCP\", \"RIGHT_MIDDLE_FINGER_PIP\",\n",
    "    \"RIGHT_MIDDLE_FINGER_DIP\", \"RIGHT_MIDDLE_FINGER_TIP\", \"RIGHT_RING_FINGER_MCP\",\n",
    "    \"RIGHT_RING_FINGER_PIP\", \"RIGHT_RING_FINGER_DIP\", \"RIGHT_RING_FINGER_TIP\",\n",
    "    \"RIGHT_PINKY_MCP\", \"RIGHT_PINKY_PIP\", \"RIGHT_PINKY_DIP\", \"RIGHT_PINKY_TIP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/home/giakhang/dev/pose_sandbox/data\"  \n",
    "SELECTED_DATE = \"2024-*\"  # Keep '*' when using glob.glob\n",
    "\n",
    "train_paths = glob.glob(os.path.join(DATA_DIR, \"{}/{}/fine_landmarks_{}_*.csv\".format(SELECTED_DATE, SELECTED_DATE, \"train\")))\n",
    "val_paths = glob.glob(os.path.join(DATA_DIR, \"{}/{}/fine_landmarks_{}_*.csv\".format(SELECTED_DATE, SELECTED_DATE, \"val\")))\n",
    "\n",
    "body_lines = [[0,2], [0, 3], [2, 4], [3, 4]]\n",
    "lefthand_lines = [[0, 1], [1, 5], [5, 6], [5, 10], [5, 22], [10, 14], [14, 18], [18, 22], \n",
    "    [6, 7], [7, 8], [8, 9], \n",
    "    [10, 11], [11, 12], [12, 13], \n",
    "    [14, 15], [15, 16], [16, 17], \n",
    "    [18, 19], [19, 20], [20, 21], \n",
    "    [22, 23], [23, 24], [24, 25]]\n",
    "train_body_distance_thres = 550\n",
    "train_leftarm_distance_thres = 550\n",
    "train_lefthand_distance_thres = 200\n",
    "val_body_distance_thres = 450\n",
    "val_leftarm_distance_thres = 450\n",
    "val_lefthand_distance_thres = 150\n",
    "\n",
    "input_scaler = MinMaxScaler()\n",
    "output_scaler = MinMaxScaler()\n",
    "\n",
    "train_dataset = HandLandmarksDataset_ANN(train_paths, \n",
    "    arm_hand_fused_names,\n",
    "    body_lines, \n",
    "    lefthand_lines, \n",
    "    train_body_distance_thres, \n",
    "    train_leftarm_distance_thres, \n",
    "    train_lefthand_distance_thres,\n",
    "    filter_outlier=True,\n",
    "    only_keep_frames_contain_lefthand=True,\n",
    "    cvt_normalized_xy_to_XY=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/giakhang/dev/pose_sandbox/Hand_pose_estimation_3D/arm_and_hand/runs/ann_left_hand/20241028/20241028-1058/output_scaler.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_scaler.fit_transform(train_dataset._inputs)\n",
    "output_scaler.fit_transform(train_dataset._outputs)\n",
    "\n",
    "input_scaler_save_path = os.path.join(SAVE_DIR, \"input_scaler.pkl\")\n",
    "output_scaler_save_path = os.path.join(SAVE_DIR, \"output_scaler.pkl\")\n",
    "\n",
    "joblib.dump(input_scaler, input_scaler_save_path)\n",
    "joblib.dump(output_scaler, output_scaler_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_scaler = LandmarksScaler(scaler_path=input_scaler_save_path)\n",
    "output_scaler = LandmarksScaler(scaler_path=output_scaler_save_path)\n",
    "\n",
    "train_dataset = HandLandmarksDataset_ANN(train_paths, \n",
    "    arm_hand_fused_names,\n",
    "    body_lines, \n",
    "    lefthand_lines, \n",
    "    train_body_distance_thres, \n",
    "    train_leftarm_distance_thres, \n",
    "    train_lefthand_distance_thres,\n",
    "    filter_outlier=True,\n",
    "    only_keep_frames_contain_lefthand=True,\n",
    "    cvt_normalized_xy_to_XY=True,\n",
    "    input_scaler=input_scaler,\n",
    "    output_scaler=output_scaler)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = HandLandmarksDataset_ANN(val_paths,\n",
    "    arm_hand_fused_names,\n",
    "    body_lines,\n",
    "    lefthand_lines,\n",
    "    val_body_distance_thres,\n",
    "    val_leftarm_distance_thres,\n",
    "    val_lefthand_distance_thres,\n",
    "    filter_outlier=True,\n",
    "    only_keep_frames_contain_lefthand=True,\n",
    "    cvt_normalized_xy_to_XY=True,\n",
    "    input_scaler=input_scaler,\n",
    "    output_scaler=output_scaler)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 50000\n",
    "current_time = datetime.now().strftime('%Y%m%d-%H%M')\n",
    "save_path = os.path.join(SAVE_DIR, \"{}_{}_layers_best.pth\".format(MODEL_NAME, NUM_HIDDEN_LAYERS))\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', \n",
    "    factor=math.sqrt(0.1), patience=1000, verbose=True, min_lr=1e-8)\n",
    "early_stopping = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved with Validation Loss: 0.0132\n",
      "Model saved with Validation Loss: 0.0125\n",
      "Model saved with Validation Loss: 0.0113\n",
      "Model saved with Validation Loss: 0.0045\n",
      "Model saved with Validation Loss: 0.0033\n",
      "Model saved with Validation Loss: 0.0033\n",
      "Model saved with Validation Loss: 0.0029\n",
      "Model saved with Validation Loss: 0.0025\n",
      "Model saved with Validation Loss: 0.0024\n",
      "Model saved with Validation Loss: 0.0023\n",
      "Epoch 50/50000, Training Loss: 0.0037\n",
      "Epoch 50/50000, Validation Loss: 0.0027\n",
      "Model saved with Validation Loss: 0.0021\n",
      "Model saved with Validation Loss: 0.0019\n",
      "Epoch 100/50000, Training Loss: 0.0035\n",
      "Epoch 100/50000, Validation Loss: 0.0077\n",
      "Model saved with Validation Loss: 0.0017\n",
      "Model saved with Validation Loss: 0.0017\n",
      "Model saved with Validation Loss: 0.0015\n",
      "Epoch 150/50000, Training Loss: 0.0034\n",
      "Epoch 150/50000, Validation Loss: 0.0048\n",
      "Epoch 200/50000, Training Loss: 0.0034\n",
      "Epoch 200/50000, Validation Loss: 0.0021\n",
      "Model saved with Validation Loss: 0.0015\n",
      "Epoch 250/50000, Training Loss: 0.0034\n",
      "Epoch 250/50000, Validation Loss: 0.0028\n",
      "Model saved with Validation Loss: 0.0015\n",
      "Model saved with Validation Loss: 0.0013\n",
      "Epoch 300/50000, Training Loss: 0.0033\n",
      "Epoch 300/50000, Validation Loss: 0.0021\n",
      "Epoch 350/50000, Training Loss: 0.0033\n",
      "Epoch 350/50000, Validation Loss: 0.0017\n",
      "Epoch 400/50000, Training Loss: 0.0033\n",
      "Epoch 400/50000, Validation Loss: 0.0022\n",
      "Model saved with Validation Loss: 0.0013\n",
      "Model saved with Validation Loss: 0.0013\n",
      "Epoch 450/50000, Training Loss: 0.0033\n",
      "Epoch 450/50000, Validation Loss: 0.0039\n",
      "Model saved with Validation Loss: 0.0012\n",
      "Epoch 500/50000, Training Loss: 0.0033\n",
      "Epoch 500/50000, Validation Loss: 0.0014\n",
      "Model saved with Validation Loss: 0.0012\n",
      "Epoch 550/50000, Training Loss: 0.0033\n",
      "Epoch 550/50000, Validation Loss: 0.0021\n",
      "Epoch 600/50000, Training Loss: 0.0032\n",
      "Epoch 600/50000, Validation Loss: 0.0018\n",
      "Epoch 650/50000, Training Loss: 0.0032\n",
      "Epoch 650/50000, Validation Loss: 0.0033\n",
      "Epoch 700/50000, Training Loss: 0.0032\n",
      "Epoch 700/50000, Validation Loss: 0.0024\n",
      "Epoch 750/50000, Training Loss: 0.0032\n",
      "Epoch 750/50000, Validation Loss: 0.0029\n",
      "Epoch 800/50000, Training Loss: 0.0032\n",
      "Epoch 800/50000, Validation Loss: 0.0014\n",
      "Model saved with Validation Loss: 0.0012\n",
      "Model saved with Validation Loss: 0.0011\n",
      "Epoch 850/50000, Training Loss: 0.0032\n",
      "Epoch 850/50000, Validation Loss: 0.0042\n",
      "Epoch 900/50000, Training Loss: 0.0032\n",
      "Epoch 900/50000, Validation Loss: 0.0015\n",
      "Model saved with Validation Loss: 0.0011\n",
      "Epoch 950/50000, Training Loss: 0.0032\n",
      "Epoch 950/50000, Validation Loss: 0.0040\n",
      "Model saved with Validation Loss: 0.0010\n",
      "Epoch 1000/50000, Training Loss: 0.0032\n",
      "Epoch 1000/50000, Validation Loss: 0.0031\n",
      "Epoch 1050/50000, Training Loss: 0.0032\n",
      "Epoch 1050/50000, Validation Loss: 0.0015\n",
      "Epoch 1100/50000, Training Loss: 0.0032\n",
      "Epoch 1100/50000, Validation Loss: 0.0018\n",
      "Epoch 1150/50000, Training Loss: 0.0032\n",
      "Epoch 1150/50000, Validation Loss: 0.0012\n",
      "Epoch 1200/50000, Training Loss: 0.0032\n",
      "Epoch 1200/50000, Validation Loss: 0.0013\n",
      "Epoch 1250/50000, Training Loss: 0.0032\n",
      "Epoch 1250/50000, Validation Loss: 0.0013\n",
      "Epoch 1300/50000, Training Loss: 0.0032\n",
      "Epoch 1300/50000, Validation Loss: 0.0016\n",
      "Epoch 1350/50000, Training Loss: 0.0032\n",
      "Epoch 1350/50000, Validation Loss: 0.0020\n",
      "Epoch 1400/50000, Training Loss: 0.0032\n",
      "Epoch 1400/50000, Validation Loss: 0.0044\n",
      "Epoch 1450/50000, Training Loss: 0.0032\n",
      "Epoch 1450/50000, Validation Loss: 0.0015\n",
      "Epoch 1500/50000, Training Loss: 0.0032\n",
      "Epoch 1500/50000, Validation Loss: 0.0022\n",
      "Epoch 1550/50000, Training Loss: 0.0032\n",
      "Epoch 1550/50000, Validation Loss: 0.0013\n",
      "Epoch 1600/50000, Training Loss: 0.0031\n",
      "Epoch 1600/50000, Validation Loss: 0.0028\n",
      "Epoch 1650/50000, Training Loss: 0.0032\n",
      "Epoch 1650/50000, Validation Loss: 0.0013\n",
      "Epoch 1700/50000, Training Loss: 0.0031\n",
      "Epoch 1700/50000, Validation Loss: 0.0016\n",
      "Epoch 1750/50000, Training Loss: 0.0032\n",
      "Epoch 1750/50000, Validation Loss: 0.0012\n",
      "Model saved with Validation Loss: 0.0010\n",
      "Epoch 1800/50000, Training Loss: 0.0032\n",
      "Epoch 1800/50000, Validation Loss: 0.0012\n",
      "Epoch 1850/50000, Training Loss: 0.0031\n",
      "Epoch 1850/50000, Validation Loss: 0.0012\n",
      "Epoch 1900/50000, Training Loss: 0.0032\n",
      "Epoch 1900/50000, Validation Loss: 0.0021\n",
      "Epoch 1950/50000, Training Loss: 0.0032\n",
      "Epoch 1950/50000, Validation Loss: 0.0013\n",
      "Epoch 2000/50000, Training Loss: 0.0032\n",
      "Epoch 2000/50000, Validation Loss: 0.0012\n",
      "Epoch 2050/50000, Training Loss: 0.0031\n",
      "Epoch 2050/50000, Validation Loss: 0.0033\n",
      "Epoch 2100/50000, Training Loss: 0.0032\n",
      "Epoch 2100/50000, Validation Loss: 0.0022\n",
      "Epoch 2150/50000, Training Loss: 0.0031\n",
      "Epoch 2150/50000, Validation Loss: 0.0013\n",
      "Epoch 2200/50000, Training Loss: 0.0032\n",
      "Epoch 2200/50000, Validation Loss: 0.0013\n",
      "Model saved with Validation Loss: 0.0010\n",
      "Epoch 2250/50000, Training Loss: 0.0032\n",
      "Epoch 2250/50000, Validation Loss: 0.0015\n",
      "Epoch 2300/50000, Training Loss: 0.0031\n",
      "Epoch 2300/50000, Validation Loss: 0.0015\n",
      "Epoch 2350/50000, Training Loss: 0.0032\n",
      "Epoch 2350/50000, Validation Loss: 0.0014\n",
      "Epoch 2400/50000, Training Loss: 0.0031\n",
      "Epoch 2400/50000, Validation Loss: 0.0018\n",
      "Epoch 2450/50000, Training Loss: 0.0031\n",
      "Epoch 2450/50000, Validation Loss: 0.0013\n",
      "Model saved with Validation Loss: 0.0010\n",
      "Epoch 2500/50000, Training Loss: 0.0031\n",
      "Epoch 2500/50000, Validation Loss: 0.0014\n",
      "Epoch 2550/50000, Training Loss: 0.0032\n",
      "Epoch 2550/50000, Validation Loss: 0.0015\n",
      "Epoch 2600/50000, Training Loss: 0.0031\n",
      "Epoch 2600/50000, Validation Loss: 0.0014\n",
      "Epoch 2650/50000, Training Loss: 0.0031\n",
      "Epoch 2650/50000, Validation Loss: 0.0017\n",
      "Epoch 2700/50000, Training Loss: 0.0031\n",
      "Epoch 2700/50000, Validation Loss: 0.0018\n",
      "Epoch 2750/50000, Training Loss: 0.0031\n",
      "Epoch 2750/50000, Validation Loss: 0.0019\n",
      "Epoch 2800/50000, Training Loss: 0.0032\n",
      "Epoch 2800/50000, Validation Loss: 0.0014\n",
      "Epoch 2850/50000, Training Loss: 0.0031\n",
      "Epoch 2850/50000, Validation Loss: 0.0014\n",
      "Epoch 2900/50000, Training Loss: 0.0031\n",
      "Epoch 2900/50000, Validation Loss: 0.0011\n",
      "Epoch 2950/50000, Training Loss: 0.0031\n",
      "Epoch 2950/50000, Validation Loss: 0.0014\n",
      "Epoch 3000/50000, Training Loss: 0.0031\n",
      "Epoch 3000/50000, Validation Loss: 0.0024\n",
      "Epoch 3050/50000, Training Loss: 0.0032\n",
      "Epoch 3050/50000, Validation Loss: 0.0015\n",
      "Epoch 3100/50000, Training Loss: 0.0031\n",
      "Epoch 3100/50000, Validation Loss: 0.0012\n",
      "Epoch 3150/50000, Training Loss: 0.0031\n",
      "Epoch 3150/50000, Validation Loss: 0.0013\n",
      "Epoch 3200/50000, Training Loss: 0.0031\n",
      "Epoch 3200/50000, Validation Loss: 0.0012\n",
      "Epoch 3250/50000, Training Loss: 0.0031\n",
      "Epoch 3250/50000, Validation Loss: 0.0011\n",
      "Epoch 3300/50000, Training Loss: 0.0031\n",
      "Epoch 3300/50000, Validation Loss: 0.0013\n",
      "Epoch 3350/50000, Training Loss: 0.0031\n",
      "Epoch 3350/50000, Validation Loss: 0.0014\n",
      "Epoch 3400/50000, Training Loss: 0.0031\n",
      "Epoch 3400/50000, Validation Loss: 0.0014\n",
      "Epoch 3450/50000, Training Loss: 0.0031\n",
      "Epoch 3450/50000, Validation Loss: 0.0022\n",
      "Epoch 03500: reducing learning rate of group 0 to 3.1623e-04.\n",
      "Epoch 3500/50000, Training Loss: 0.0031\n",
      "Epoch 3500/50000, Validation Loss: 0.0016\n",
      "Epoch 3550/50000, Training Loss: 0.0031\n",
      "Epoch 3550/50000, Validation Loss: 0.0016\n",
      "Epoch 3600/50000, Training Loss: 0.0031\n",
      "Epoch 3600/50000, Validation Loss: 0.0012\n",
      "Epoch 3650/50000, Training Loss: 0.0031\n",
      "Epoch 3650/50000, Validation Loss: 0.0014\n",
      "Epoch 3700/50000, Training Loss: 0.0031\n",
      "Epoch 3700/50000, Validation Loss: 0.0014\n",
      "Epoch 3750/50000, Training Loss: 0.0031\n",
      "Epoch 3750/50000, Validation Loss: 0.0014\n",
      "Epoch 3800/50000, Training Loss: 0.0031\n",
      "Epoch 3800/50000, Validation Loss: 0.0018\n",
      "Epoch 3850/50000, Training Loss: 0.0031\n",
      "Epoch 3850/50000, Validation Loss: 0.0019\n",
      "Epoch 3900/50000, Training Loss: 0.0031\n",
      "Epoch 3900/50000, Validation Loss: 0.0013\n",
      "Epoch 3950/50000, Training Loss: 0.0031\n",
      "Epoch 3950/50000, Validation Loss: 0.0014\n",
      "Epoch 4000/50000, Training Loss: 0.0031\n",
      "Epoch 4000/50000, Validation Loss: 0.0016\n",
      "Epoch 4050/50000, Training Loss: 0.0031\n",
      "Epoch 4050/50000, Validation Loss: 0.0015\n",
      "Epoch 4100/50000, Training Loss: 0.0031\n",
      "Epoch 4100/50000, Validation Loss: 0.0014\n",
      "Epoch 4150/50000, Training Loss: 0.0031\n",
      "Epoch 4150/50000, Validation Loss: 0.0016\n",
      "Epoch 4200/50000, Training Loss: 0.0031\n",
      "Epoch 4200/50000, Validation Loss: 0.0016\n",
      "Epoch 4250/50000, Training Loss: 0.0031\n",
      "Epoch 4250/50000, Validation Loss: 0.0016\n",
      "Epoch 4300/50000, Training Loss: 0.0031\n",
      "Epoch 4300/50000, Validation Loss: 0.0016\n",
      "Epoch 4350/50000, Training Loss: 0.0031\n",
      "Epoch 4350/50000, Validation Loss: 0.0015\n",
      "Epoch 4400/50000, Training Loss: 0.0031\n",
      "Epoch 4400/50000, Validation Loss: 0.0016\n",
      "Epoch 4450/50000, Training Loss: 0.0031\n",
      "Epoch 4450/50000, Validation Loss: 0.0015\n",
      "Epoch 4500/50000, Training Loss: 0.0031\n",
      "Epoch 4500/50000, Validation Loss: 0.0015\n",
      "Epoch 04501: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 4550/50000, Training Loss: 0.0030\n",
      "Epoch 4550/50000, Validation Loss: 0.0014\n",
      "Epoch 4600/50000, Training Loss: 0.0030\n",
      "Epoch 4600/50000, Validation Loss: 0.0014\n",
      "Epoch 4650/50000, Training Loss: 0.0030\n",
      "Epoch 4650/50000, Validation Loss: 0.0014\n",
      "Epoch 4700/50000, Training Loss: 0.0030\n",
      "Epoch 4700/50000, Validation Loss: 0.0015\n",
      "Epoch 4750/50000, Training Loss: 0.0031\n",
      "Epoch 4750/50000, Validation Loss: 0.0014\n",
      "Epoch 4800/50000, Training Loss: 0.0030\n",
      "Epoch 4800/50000, Validation Loss: 0.0015\n",
      "Epoch 4850/50000, Training Loss: 0.0030\n",
      "Epoch 4850/50000, Validation Loss: 0.0015\n",
      "Epoch 4900/50000, Training Loss: 0.0030\n",
      "Epoch 4900/50000, Validation Loss: 0.0014\n",
      "Epoch 4950/50000, Training Loss: 0.0030\n",
      "Epoch 4950/50000, Validation Loss: 0.0014\n",
      "Epoch 5000/50000, Training Loss: 0.0030\n",
      "Epoch 5000/50000, Validation Loss: 0.0017\n",
      "Epoch 5050/50000, Training Loss: 0.0030\n",
      "Epoch 5050/50000, Validation Loss: 0.0015\n",
      "Epoch 5100/50000, Training Loss: 0.0030\n",
      "Epoch 5100/50000, Validation Loss: 0.0015\n",
      "Epoch 5150/50000, Training Loss: 0.0030\n",
      "Epoch 5150/50000, Validation Loss: 0.0015\n",
      "Epoch 5200/50000, Training Loss: 0.0030\n",
      "Epoch 5200/50000, Validation Loss: 0.0014\n",
      "Epoch 5250/50000, Training Loss: 0.0030\n",
      "Epoch 5250/50000, Validation Loss: 0.0014\n",
      "Epoch 5300/50000, Training Loss: 0.0030\n",
      "Epoch 5300/50000, Validation Loss: 0.0014\n",
      "Epoch 5350/50000, Training Loss: 0.0030\n",
      "Epoch 5350/50000, Validation Loss: 0.0013\n",
      "Epoch 5400/50000, Training Loss: 0.0030\n",
      "Epoch 5400/50000, Validation Loss: 0.0015\n",
      "Epoch 5450/50000, Training Loss: 0.0030\n",
      "Epoch 5450/50000, Validation Loss: 0.0015\n",
      "Epoch 5500/50000, Training Loss: 0.0030\n",
      "Epoch 5500/50000, Validation Loss: 0.0014\n",
      "Epoch 05502: reducing learning rate of group 0 to 3.1623e-05.\n",
      "Epoch 5550/50000, Training Loss: 0.0030\n",
      "Epoch 5550/50000, Validation Loss: 0.0015\n",
      "Epoch 5600/50000, Training Loss: 0.0030\n",
      "Epoch 5600/50000, Validation Loss: 0.0014\n",
      "Epoch 5650/50000, Training Loss: 0.0030\n",
      "Epoch 5650/50000, Validation Loss: 0.0014\n",
      "Epoch 5700/50000, Training Loss: 0.0030\n",
      "Epoch 5700/50000, Validation Loss: 0.0015\n",
      "Epoch 5750/50000, Training Loss: 0.0030\n",
      "Epoch 5750/50000, Validation Loss: 0.0016\n",
      "Epoch 5800/50000, Training Loss: 0.0030\n",
      "Epoch 5800/50000, Validation Loss: 0.0014\n",
      "Epoch 5850/50000, Training Loss: 0.0030\n",
      "Epoch 5850/50000, Validation Loss: 0.0014\n",
      "Epoch 5900/50000, Training Loss: 0.0030\n",
      "Epoch 5900/50000, Validation Loss: 0.0014\n",
      "Epoch 5950/50000, Training Loss: 0.0030\n",
      "Epoch 5950/50000, Validation Loss: 0.0015\n",
      "Epoch 6000/50000, Training Loss: 0.0030\n",
      "Epoch 6000/50000, Validation Loss: 0.0014\n",
      "Epoch 6050/50000, Training Loss: 0.0030\n",
      "Epoch 6050/50000, Validation Loss: 0.0015\n",
      "Epoch 6100/50000, Training Loss: 0.0030\n",
      "Epoch 6100/50000, Validation Loss: 0.0014\n",
      "Epoch 6150/50000, Training Loss: 0.0030\n",
      "Epoch 6150/50000, Validation Loss: 0.0015\n",
      "Epoch 6200/50000, Training Loss: 0.0030\n",
      "Epoch 6200/50000, Validation Loss: 0.0015\n",
      "Epoch 6250/50000, Training Loss: 0.0030\n",
      "Epoch 6250/50000, Validation Loss: 0.0016\n",
      "Epoch 6300/50000, Training Loss: 0.0030\n",
      "Epoch 6300/50000, Validation Loss: 0.0016\n",
      "Epoch 6350/50000, Training Loss: 0.0030\n",
      "Epoch 6350/50000, Validation Loss: 0.0014\n",
      "Epoch 6400/50000, Training Loss: 0.0030\n",
      "Epoch 6400/50000, Validation Loss: 0.0015\n",
      "Epoch 6450/50000, Training Loss: 0.0030\n",
      "Epoch 6450/50000, Validation Loss: 0.0014\n",
      "Epoch 6500/50000, Training Loss: 0.0030\n",
      "Epoch 6500/50000, Validation Loss: 0.0016\n",
      "Epoch 06503: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 6550/50000, Training Loss: 0.0030\n",
      "Epoch 6550/50000, Validation Loss: 0.0015\n",
      "Epoch 6600/50000, Training Loss: 0.0030\n",
      "Epoch 6600/50000, Validation Loss: 0.0015\n",
      "Epoch 6650/50000, Training Loss: 0.0030\n",
      "Epoch 6650/50000, Validation Loss: 0.0015\n",
      "Epoch 6700/50000, Training Loss: 0.0030\n",
      "Epoch 6700/50000, Validation Loss: 0.0015\n",
      "Epoch 6750/50000, Training Loss: 0.0030\n",
      "Epoch 6750/50000, Validation Loss: 0.0014\n",
      "Epoch 6800/50000, Training Loss: 0.0030\n",
      "Epoch 6800/50000, Validation Loss: 0.0014\n",
      "Epoch 6850/50000, Training Loss: 0.0030\n",
      "Epoch 6850/50000, Validation Loss: 0.0014\n",
      "Epoch 6900/50000, Training Loss: 0.0030\n",
      "Epoch 6900/50000, Validation Loss: 0.0015\n",
      "Epoch 6950/50000, Training Loss: 0.0030\n",
      "Epoch 6950/50000, Validation Loss: 0.0015\n",
      "Epoch 7000/50000, Training Loss: 0.0030\n",
      "Epoch 7000/50000, Validation Loss: 0.0015\n",
      "Epoch 7050/50000, Training Loss: 0.0030\n",
      "Epoch 7050/50000, Validation Loss: 0.0014\n",
      "Epoch 7100/50000, Training Loss: 0.0030\n",
      "Epoch 7100/50000, Validation Loss: 0.0015\n",
      "Epoch 7150/50000, Training Loss: 0.0030\n",
      "Epoch 7150/50000, Validation Loss: 0.0014\n",
      "Epoch 7200/50000, Training Loss: 0.0030\n",
      "Epoch 7200/50000, Validation Loss: 0.0014\n",
      "Epoch 7250/50000, Training Loss: 0.0030\n",
      "Epoch 7250/50000, Validation Loss: 0.0014\n",
      "Epoch 7300/50000, Training Loss: 0.0030\n",
      "Epoch 7300/50000, Validation Loss: 0.0014\n",
      "Epoch 7350/50000, Training Loss: 0.0030\n",
      "Epoch 7350/50000, Validation Loss: 0.0014\n",
      "Epoch 7400/50000, Training Loss: 0.0030\n",
      "Epoch 7400/50000, Validation Loss: 0.0015\n",
      "Epoch 7450/50000, Training Loss: 0.0030\n",
      "Epoch 7450/50000, Validation Loss: 0.0015\n",
      "Epoch 7500/50000, Training Loss: 0.0030\n",
      "Epoch 7500/50000, Validation Loss: 0.0016\n",
      "Epoch 07504: reducing learning rate of group 0 to 3.1623e-06.\n",
      "Epoch 7550/50000, Training Loss: 0.0030\n",
      "Epoch 7550/50000, Validation Loss: 0.0016\n",
      "Epoch 7600/50000, Training Loss: 0.0030\n",
      "Epoch 7600/50000, Validation Loss: 0.0014\n",
      "Epoch 7650/50000, Training Loss: 0.0030\n",
      "Epoch 7650/50000, Validation Loss: 0.0015\n",
      "Epoch 7700/50000, Training Loss: 0.0030\n",
      "Epoch 7700/50000, Validation Loss: 0.0014\n",
      "Epoch 7750/50000, Training Loss: 0.0030\n",
      "Epoch 7750/50000, Validation Loss: 0.0014\n",
      "Epoch 7800/50000, Training Loss: 0.0030\n",
      "Epoch 7800/50000, Validation Loss: 0.0015\n",
      "Epoch 7850/50000, Training Loss: 0.0030\n",
      "Epoch 7850/50000, Validation Loss: 0.0014\n",
      "Epoch 7900/50000, Training Loss: 0.0030\n",
      "Epoch 7900/50000, Validation Loss: 0.0014\n",
      "Epoch 7950/50000, Training Loss: 0.0030\n",
      "Epoch 7950/50000, Validation Loss: 0.0014\n",
      "Epoch 8000/50000, Training Loss: 0.0030\n",
      "Epoch 8000/50000, Validation Loss: 0.0014\n",
      "Epoch 8050/50000, Training Loss: 0.0030\n",
      "Epoch 8050/50000, Validation Loss: 0.0015\n",
      "Epoch 8100/50000, Training Loss: 0.0030\n",
      "Epoch 8100/50000, Validation Loss: 0.0014\n",
      "Epoch 8150/50000, Training Loss: 0.0030\n",
      "Epoch 8150/50000, Validation Loss: 0.0016\n",
      "Epoch 8200/50000, Training Loss: 0.0030\n",
      "Epoch 8200/50000, Validation Loss: 0.0015\n",
      "Epoch 8250/50000, Training Loss: 0.0030\n",
      "Epoch 8250/50000, Validation Loss: 0.0014\n",
      "Epoch 8300/50000, Training Loss: 0.0030\n",
      "Epoch 8300/50000, Validation Loss: 0.0016\n",
      "Epoch 8350/50000, Training Loss: 0.0030\n",
      "Epoch 8350/50000, Validation Loss: 0.0016\n",
      "Epoch 8400/50000, Training Loss: 0.0030\n",
      "Epoch 8400/50000, Validation Loss: 0.0015\n",
      "Epoch 8450/50000, Training Loss: 0.0030\n",
      "Epoch 8450/50000, Validation Loss: 0.0015\n",
      "Epoch 8500/50000, Training Loss: 0.0030\n",
      "Epoch 8500/50000, Validation Loss: 0.0015\n",
      "Epoch 08505: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 8550/50000, Training Loss: 0.0030\n",
      "Epoch 8550/50000, Validation Loss: 0.0015\n",
      "Epoch 8600/50000, Training Loss: 0.0030\n",
      "Epoch 8600/50000, Validation Loss: 0.0014\n",
      "Epoch 8650/50000, Training Loss: 0.0030\n",
      "Epoch 8650/50000, Validation Loss: 0.0015\n",
      "Epoch 8700/50000, Training Loss: 0.0030\n",
      "Epoch 8700/50000, Validation Loss: 0.0014\n",
      "Epoch 8750/50000, Training Loss: 0.0030\n",
      "Epoch 8750/50000, Validation Loss: 0.0014\n",
      "Epoch 8800/50000, Training Loss: 0.0030\n",
      "Epoch 8800/50000, Validation Loss: 0.0015\n",
      "Epoch 8850/50000, Training Loss: 0.0030\n",
      "Epoch 8850/50000, Validation Loss: 0.0014\n",
      "Epoch 8900/50000, Training Loss: 0.0030\n",
      "Epoch 8900/50000, Validation Loss: 0.0015\n",
      "Epoch 8950/50000, Training Loss: 0.0030\n",
      "Epoch 8950/50000, Validation Loss: 0.0015\n",
      "Epoch 9000/50000, Training Loss: 0.0030\n",
      "Epoch 9000/50000, Validation Loss: 0.0015\n",
      "Epoch 9050/50000, Training Loss: 0.0030\n",
      "Epoch 9050/50000, Validation Loss: 0.0014\n",
      "Epoch 9100/50000, Training Loss: 0.0030\n",
      "Epoch 9100/50000, Validation Loss: 0.0014\n",
      "Epoch 9150/50000, Training Loss: 0.0030\n",
      "Epoch 9150/50000, Validation Loss: 0.0014\n",
      "Epoch 9200/50000, Training Loss: 0.0030\n",
      "Epoch 9200/50000, Validation Loss: 0.0014\n",
      "Epoch 9250/50000, Training Loss: 0.0030\n",
      "Epoch 9250/50000, Validation Loss: 0.0014\n",
      "Epoch 9300/50000, Training Loss: 0.0030\n",
      "Epoch 9300/50000, Validation Loss: 0.0013\n",
      "Epoch 9350/50000, Training Loss: 0.0030\n",
      "Epoch 9350/50000, Validation Loss: 0.0014\n",
      "Epoch 9400/50000, Training Loss: 0.0030\n",
      "Epoch 9400/50000, Validation Loss: 0.0014\n",
      "Epoch 9450/50000, Training Loss: 0.0030\n",
      "Epoch 9450/50000, Validation Loss: 0.0015\n",
      "Epoch 9500/50000, Training Loss: 0.0030\n",
      "Epoch 9500/50000, Validation Loss: 0.0014\n",
      "Epoch 09506: reducing learning rate of group 0 to 3.1623e-07.\n",
      "Epoch 9550/50000, Training Loss: 0.0030\n",
      "Epoch 9550/50000, Validation Loss: 0.0015\n",
      "Epoch 9600/50000, Training Loss: 0.0030\n",
      "Epoch 9600/50000, Validation Loss: 0.0015\n",
      "Epoch 9650/50000, Training Loss: 0.0030\n",
      "Epoch 9650/50000, Validation Loss: 0.0015\n",
      "Epoch 9700/50000, Training Loss: 0.0030\n",
      "Epoch 9700/50000, Validation Loss: 0.0015\n",
      "Epoch 9750/50000, Training Loss: 0.0030\n",
      "Epoch 9750/50000, Validation Loss: 0.0014\n",
      "Epoch 9800/50000, Training Loss: 0.0030\n",
      "Epoch 9800/50000, Validation Loss: 0.0014\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m train_model(model, \n\u001b[1;32m      2\u001b[0m     train_dataloader, \n\u001b[1;32m      3\u001b[0m     val_dataloader, \n\u001b[1;32m      4\u001b[0m     optimizer, \n\u001b[1;32m      5\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs, \n\u001b[1;32m      6\u001b[0m     save_path\u001b[38;5;241m=\u001b[39msave_path,\n\u001b[1;32m      7\u001b[0m     early_stopping\u001b[38;5;241m=\u001b[39mearly_stopping,\n\u001b[1;32m      8\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[1;32m      9\u001b[0m     writer\u001b[38;5;241m=\u001b[39mwriter,\n\u001b[1;32m     10\u001b[0m     log_seq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     11\u001b[0m     train_left_arm_hand_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m     weight_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m     weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m)\n\u001b[1;32m     15\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/dev/pose_sandbox/Hand_pose_estimation_3D/arm_and_hand/train_ann_no_intrinsics.py:90\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, num_epochs, save_path, early_stopping, scheduler, writer, log_seq, weight_idx, weight, train_left_arm_hand_only)\u001b[0m\n\u001b[1;32m     86\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     88\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \n\u001b[0;32m---> 90\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[1;32m     91\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mmdeploy/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mmdeploy/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/mmdeploy/lib/python3.11/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     adam(\n\u001b[1;32m    164\u001b[0m         params_with_grad,\n\u001b[1;32m    165\u001b[0m         grads,\n\u001b[1;32m    166\u001b[0m         exp_avgs,\n\u001b[1;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    169\u001b[0m         state_steps,\n\u001b[1;32m    170\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    171\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    172\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    173\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    174\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    175\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    176\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    177\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    178\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    179\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    180\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    181\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    182\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/mmdeploy/lib/python3.11/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m func(params,\n\u001b[1;32m    312\u001b[0m      grads,\n\u001b[1;32m    313\u001b[0m      exp_avgs,\n\u001b[1;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    316\u001b[0m      state_steps,\n\u001b[1;32m    317\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    318\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    319\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    320\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    321\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    322\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    323\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    324\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    325\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    326\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    327\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[0;32m~/anaconda3/envs/mmdeploy/lib/python3.11/site-packages/torch/optim/adam.py:565\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    563\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_exp_avg_sqs)\n\u001b[1;32m    567\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[1;32m    568\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train_model(model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    optimizer, \n",
    "    num_epochs=num_epochs, \n",
    "    save_path=save_path,\n",
    "    early_stopping=early_stopping,\n",
    "    scheduler=scheduler,\n",
    "    writer=writer,\n",
    "    log_seq=50,\n",
    "    train_left_arm_hand_only=True,\n",
    "    weight_idx=None,\n",
    "    weight=1.)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmdeploy",
   "language": "python",
   "name": "mmdeploy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
